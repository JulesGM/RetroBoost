Fixed loss thing
    check ddp backend

Accuracy increases but loss goes up! We trade off a bit of average loss for strenght with hard samples


wd 0.01:
    [worse than baseline] test non-baseline with const epsilon .5 wd 0.01 
    [better than .5 in the begining, then equal]test non-baseline with const epsilon .25
    [bad]test non-baseline with const epsilon .75
    [bad]test non-baseline with const epsilon .875
    [equally as bad] INVERSE test non-baseline with const epsilon .875
    test non-baseline with const epsilon .125
    test non-baseline with linear epsilon .5 1000
    test non-baseline with linear epsilon .2 1000
    test non-baseline with linear epsilon .8 1000
    test non-baseline with linear epsilon .5 2000
    test non-baseline with linear epsilon .2 2000
    test non-baseline with linear epsilon .8 2000
code language modeling

test non-baseline with linear epsilon <depends on prev results>

test clm