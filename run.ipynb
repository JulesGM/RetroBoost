{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "from beartype import beartype\n",
    "import datasets\n",
    "import numpy as np\n",
    "import more_itertools\n",
    "import queue\n",
    "import rich\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import wandb\n",
    "\n",
    "TokenizerType = transformers.tokenization_utils_fast.PreTrainedTokenizerFast\n",
    "\n",
    "class BaseEpsilonScheduler(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def __call__(self):\n",
    "        pass\n",
    "\n",
    "class LinearEpsilonScheduler(BaseEpsilonScheduler):\n",
    "    def __init__(self, epsilon, num_steps):\n",
    "        self.epsilon = epsilon\n",
    "        self.num_steps = num_steps\n",
    "        self.epoch = 0\n",
    "\n",
    "    def __call__(self):\n",
    "        self.epoch += 1\n",
    "        epsilon = min(self.epsilon * (1 - self.epoch / self.num_epochs), 1)\n",
    "        wandb.log({\"epsilon\": epsilon})\n",
    "        wandb.log({\"epsilon_num_steps\": self.num_steps})\n",
    "        return epsilon\n",
    "\n",
    "class ConstantEpsilonScheduler(BaseEpsilonScheduler):\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __call__(self):\n",
    "        epsilon = self.epsilon\n",
    "        wandb.log({\"epsilon\": epsilon})\n",
    "        return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/beartype/_util/hint/pep/utilpeptest.py:396: BeartypeDecorHintPepDeprecatedWarning: Type hint typing.List[str] deprecated by PEP 585. To resolve this, globally replace this hint by the equivalent PEP 585 type hint (e.g., \"typing.List[int]\" by \"list[int]\"). See also:\n",
      "    https://www.python.org/dev/peps/pep-0585\n",
      "  warn(warning_message, BeartypeDecorHintPepDeprecatedWarning)\n",
      "/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/beartype/_util/hint/pep/utilpeptest.py:396: BeartypeDecorHintPepDeprecatedWarning: Type hint typing.List[int] deprecated by PEP 585. To resolve this, globally replace this hint by the equivalent PEP 585 type hint (e.g., \"typing.List[int]\" by \"list[int]\"). See also:\n",
      "    https://www.python.org/dev/peps/pep-0585\n",
      "  warn(warning_message, BeartypeDecorHintPepDeprecatedWarning)\n"
     ]
    }
   ],
   "source": [
    "class BaseRetriever(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def retrieve(self, query_ids, query_index):\n",
    "        pass\n",
    "\n",
    "\n",
    "class StupidRetriever(BaseRetriever): \n",
    "    @beartype\n",
    "    def __init__(\n",
    "        self, *, model, tokenizer: TokenizerType, device: Union[int, str], \n",
    "        train_vectors: torch.Tensor, train_samples: List[str], train_labels: List[int]\n",
    "    ):\n",
    "    \n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.train_vectors = train_vectors\n",
    "        self.train_samples = train_samples\n",
    "        self.train_labels = train_labels\n",
    "        self.classification_ids_to_idx = {}\n",
    "\n",
    "        for i, sample in enumerate(train_samples):\n",
    "            encoded = tokenizer.encode(sample, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "            self.classification_ids_to_idx[encoded] = i\n",
    "\n",
    "    def retrieve(self, query_ids, query_index):\n",
    "        # Get the representation\n",
    "        representation = self.train_vectors[query_index]\n",
    "        with torch.inference_mode():\n",
    "            # Compute the inner products\n",
    "            scores = torch.matmul(representation, self.train_vectors.t())\n",
    "            # Get the top 2 results, to potentially exclude the sample itself.\n",
    "            topk = torch.topk(scores, k=2, dim=-1)\n",
    "        topk = topk.indices.cpu().numpy()\n",
    "        \n",
    "        for retrieved_idx in topk:\n",
    "            if retrieved_idx != query_index:\n",
    "                return self.train_samples[retrieved_idx], self.train_labels[retrieved_idx], retrieved_idx\n",
    "        \n",
    "\n",
    "# build train vectors\n",
    "@beartype\n",
    "def make_retrival_model_and_vectors(\n",
    "    retriever_name: str, path_to_vectors: Union[str, Path], device: int, dataset_name: str\n",
    "):\n",
    "    \"\"\"We expect the dir to have the following structure:\n",
    "    - config.json\n",
    "    - train_samples.json \n",
    "    - train_vectors.npy\n",
    "    \"\"\"    \n",
    "    # Make some checks\n",
    "    config =  json.loads((path_to_vectors / \"config.json\").read_text())\n",
    "    assert dataset_name == config[\"dataset_name\"], (dataset_name, config[\"dataset_name\"])\n",
    "    assert retriever_name == config[\"retriever_name\"], (retriever_name, config[\"retriever_name\"])\n",
    "\n",
    "    retriever_model = transformers.AutoModel.from_pretrained(retriever_name)\n",
    "    retriever_tokenizer = transformers.AutoTokenizer.from_pretrained(retriever_name)\n",
    "\n",
    "    with open(path_to_vectors / \"train_samples.json\") as f:\n",
    "        train_samples = json.load(f)\n",
    "        \n",
    "    vectors = torch.tensor(np.load(path_to_vectors / \"train_vectors.npy\")).to(device)\n",
    "    retriever = StupidRetriever(\n",
    "        model=retriever_model, \n",
    "        tokenizer=retriever_tokenizer, \n",
    "        device=device, \n",
    "        train_vectors=vectors, \n",
    "        train_samples=train_samples[\"inputs\"],\n",
    "        train_labels=train_samples[\"labels\"],\n",
    "    )\n",
    "    \n",
    "    return retriever\n",
    "\n",
    "\n",
    "@dataclass(order=True)\n",
    "class PrioritizedItem:\n",
    "    priority: int\n",
    "    item: Any=field(compare=False)\n",
    "\n",
    "\n",
    "class BoostingIterator(torch.utils.data.IterableDataset):\n",
    "    @beartype\n",
    "    def __init__(\n",
    "        self, \n",
    "        *args, \n",
    "        dataset: torch.utils.data.Dataset, \n",
    "        retriever_client: BaseRetriever, \n",
    "        classifier: nn.Module, seed: int, \n",
    "        classification_device: Union[int, str], \n",
    "        classification_tokenizer: TokenizerType, \n",
    "        retriever_device: Union[int, str],\n",
    "        epsilon_scheduler: BaseEpsilonScheduler, \n",
    "        loss_ema_alpha: float, \n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.dataset = dataset.map(\n",
    "            lambda example, idx:{\"index\": idx}, with_indices=True\n",
    "        ).shuffle(seed=seed)\n",
    "        self.priority_queue = queue.PriorityQueue()\n",
    "        self.retriever_client = retriever_client\n",
    "        self.epsilon_scheduler = epsilon_scheduler\n",
    "        self.randomizer = np.random.RandomState(seed)\n",
    "        self.seed = seed\n",
    "        self.dataset_iter = None\n",
    "        self.classifier = classifier\n",
    "        self.classification_tokenizer = classification_tokenizer\n",
    "        self.classification_device = classification_device\n",
    "        self.retriever_device = retriever_device\n",
    "        self.loss_moving_average = None\n",
    "        self.loss_ema_alpha = loss_ema_alpha\n",
    "\n",
    "        # assert mode in [\"epsilon_priority_no_reset\", \"pure_sampled\", \"epsilon_sampled\"], mode\n",
    "\n",
    "    def push_score(self, inputs, loss):\n",
    "        average_loss = loss.mean()\n",
    "        if self.loss_moving_average is None:\n",
    "            self.loss_moving_average = average_loss\n",
    "        else:\n",
    "            self.loss_moving_average = (\n",
    "                self.loss_ema_alpha * self.loss_moving_average + (1 - self.loss_ema_alpha) * average_loss\n",
    "            )\n",
    "\n",
    "        for input_, mask, loss_, index in (\n",
    "            more_itertools.zip_equal(inputs[\"input_ids\"], inputs[\"attention_mask\"], loss, inputs[\"index\"])\n",
    "        ):\n",
    "            assert loss_.shape == torch.Size([]), loss_.shape\n",
    "            self.priority_queue.put(\n",
    "                PrioritizedItem(\n",
    "                    priority= -loss_.detach().cpu().numpy() / self.loss_moving_average, \n",
    "                    item=dict(input_ids=input_, attention_mask=mask, index=index)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        rich.print(\"[bold green]ITER[/]\")\n",
    "        self.dataset = self.dataset.shuffle(seed=self.seed)\n",
    "        self.dataset_iter = iter(self.dataset)\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        \"\"\" This is where the sampling happens.\n",
    "        \"\"\"\n",
    "\n",
    "        # Test if we have a sample and if we pass the epsilon threshold\n",
    "        empty = self.priority_queue.empty()\n",
    "        rand = self.randomizer.rand()\n",
    "        if not empty and rand < self.epsilon_scheduler():\n",
    "            # pull a sample from the priority queue\n",
    "            sample = self.priority_queue.get().item\n",
    "\n",
    "            # We retrieve the next sample.\n",
    "            input_, next_label, index = self.retriever_client.retrieve(\n",
    "                sample[\"input_ids\"], sample[\"index\"]\n",
    "            )\n",
    "            next_sample = dict(text=input_, label=next_label, index=index)\n",
    "        else:\n",
    "            next_sample = next(self.dataset_iter)  # We raise here if we have no more samples in the dataset\n",
    "            assert next_sample.keys() == {\"text\", \"label\", \"index\"}, next_sample.keys()\n",
    "\n",
    "        tokenized = self.classification_tokenizer.encode_plus(\n",
    "            next_sample[\"text\"].strip(), \n",
    "            truncation=True, \n",
    "            padding=True,\n",
    "        )\n",
    "\n",
    "        # text is not needed anymore\n",
    "        del next_sample[\"text\"]\n",
    "        assert len(tokenized.keys() & next_sample.keys()) == 0, (tokenized.keys(), next_sample.keys()) \n",
    "        return dict(**tokenized, **next_sample)\n",
    "\n",
    "\n",
    "class BoostingTrainer(transformers.Trainer):\n",
    "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "        Subclass and override to inject custom behavior.\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "                The dictionary will be unpacked before being fed to the model. \n",
    "                Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "        Return:\n",
    "            `torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        index = inputs[\"index\"]\n",
    "        # Compute loss doesn't work with extra arguments.\n",
    "        del inputs[\"index\"]\n",
    "\n",
    "        with self.autocast_smart_context_manager():\n",
    "            # Get the loss\n",
    "            loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            # Mean over per gpu averages\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "        # This is ignored in the priority queue computation\n",
    "        if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:\n",
    "            # Deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "        if self.do_grad_scaling:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        elif self.use_apex:\n",
    "            with torch.cuda.amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        elif self.deepspeed:\n",
    "            # loss gets scaled under gradient_accumulation_steps in deepspeed\n",
    "            loss = self.deepspeed.backward(loss)\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        loss = loss.detach()\n",
    "\n",
    "        # Addition for RetroBoost\n",
    "        # Make sure the losses are similar, then push them to the priority queue\n",
    "        # Put index back in\n",
    "\n",
    "        inputs[\"index\"] = index\n",
    "        computed_loss = torch.nn.functional.cross_entropy(outputs.logits.detach(), inputs[\"labels\"].detach(), reduction=\"mean\")\n",
    "        loss_per_sample = torch.nn.functional.cross_entropy(outputs.logits.detach(), inputs[\"labels\"].detach(), reduction=\"none\")\n",
    "        \n",
    "        assert torch.allclose(loss, computed_loss, atol=0.1)\n",
    "        self.get_train_dataloader().dataset.push_score(inputs, loss_per_sample)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mretroboost\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/retroboost/RetroBoost/runs/vo2eaz0w\" target=\"_blank\">laced-sky-1</a></strong> to <a href=\"https://wandb.ai/retroboost/RetroBoost\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/home/mila/g/gagnonju/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea5c672219343229d9e69510a08c7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/home/mila/g/gagnonju/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239f0cc2cd8e4bf581e765ba871e825d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e370d5d1bfe641dbb7ee5abd7b79ec79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2becd116ab4251ad406b708d4c91b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RETRIEVER_NAME = \"facebook/contriever\"\n",
    "DATASET_NAME = \"ag_news\"\n",
    "PATH_TO_VECTORS = Path(f\"./vectors_{DATASET_NAME}_{RETRIEVER_NAME.split('/')[-1]}/\")\n",
    "CLASSIFIER_NAME = \"roberta-base\"\n",
    "CLASSIFIER_BATCH_SIZE = 40\n",
    "EPSILON_SCHEDULER_TYPE = \"constant\"\n",
    "EPSILON_SCHEDULER_CONFIG = dict(\n",
    "    epsilon=0.5,\n",
    ")\n",
    "LOSS_EMA_ALPHA = 0.5\n",
    "\n",
    "REGULAR_TRAINER = True\n",
    "CLASSIFIER_DEVICE = 1\n",
    "RETRIEVER_DEVICE = 2\n",
    "SEED = 0\n",
    "\n",
    "###############################################################################\n",
    "# Fast setup \n",
    "###############################################################################\n",
    "config = dict(\n",
    "        retriever_name=RETRIEVER_NAME,\n",
    "        dataset_name=DATASET_NAME,\n",
    "        classifier_name=CLASSIFIER_NAME,\n",
    "        regular_trainer=REGULAR_TRAINER,\n",
    "        loss_ema_alpha=LOSS_EMA_ALPHA,\n",
    "        epsilon=dict(\n",
    "            scheduler_type=EPSILON_SCHEDULER_TYPE,\n",
    "            scheduler_config=EPSILON_SCHEDULER_CONFIG,\n",
    "        )\n",
    "    )\n",
    "\n",
    "wandb.init(\n",
    "    config=config,\n",
    "    project=\"RetroBoost\", \n",
    "    entity=\"retroboost\",\n",
    ")\n",
    "\n",
    "EPSILON_SCHEDULER_TYPE_MAP = dict(\n",
    "    constant=ConstantEpsilonScheduler,\n",
    ")\n",
    "\n",
    "# Random seeds. \n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "dataset = datasets.load_dataset(DATASET_NAME)\n",
    "ALL_LABELS = set(dataset[\"train\"][\"label\"])\n",
    "NUM_LABELS = len(ALL_LABELS)\n",
    "assert ALL_LABELS == set(range(NUM_LABELS))\n",
    "\n",
    "classifier_name = CLASSIFIER_NAME\n",
    "dataset_name = DATASET_NAME\n",
    "regular_trainer = REGULAR_TRAINER\n",
    "\n",
    "\n",
    "classifier = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    classifier_name, num_labels=NUM_LABELS\n",
    ")\n",
    "classifier_tokenizer = transformers.AutoTokenizer.from_pretrained(classifier_name)\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "dataset = datasets.load_dataset(dataset_name)\n",
    "tokenized_training = dataset[\"train\"].map(\n",
    "    lambda examples: preprocess_function(examples, classifier_tokenizer), \n",
    "    batched=True\n",
    ")\n",
    "\n",
    "tokenized_validation = dataset[\"test\"].map(\n",
    "    lambda examples: preprocess_function(examples, classifier_tokenizer), \n",
    "    batched=True\n",
    ")\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    eval_steps=499,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=CLASSIFIER_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=int(CLASSIFIER_BATCH_SIZE * 1.5),\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = make_retrival_model_and_vectors(\n",
    "    retriever_name=RETRIEVER_NAME, \n",
    "    path_to_vectors=PATH_TO_VECTORS, \n",
    "    device=RETRIEVER_DEVICE, \n",
    "    dataset_name=DATASET_NAME,\n",
    ")\n",
    "retriever_client = retriever\n",
    "\n",
    "if regular_trainer:\n",
    "    trainer = transformers.Trainer(\n",
    "        model=classifier.to(CLASSIFIER_DEVICE), \n",
    "        args=training_args, \n",
    "        tokenizer=classifier_tokenizer, \n",
    "        train_dataset=tokenized_training, \n",
    "        eval_dataset=tokenized_validation,\n",
    "        data_collator=transformers.DataCollatorWithPadding(\n",
    "            tokenizer=classifier_tokenizer\n",
    "        ),\n",
    "    )\n",
    "else:\n",
    "    tokenized_training = BoostingIterator(\n",
    "        dataset=dataset[\"train\"], \n",
    "        retriever_client=retriever_client, \n",
    "        classifier=classifier, \n",
    "        epsilon_scheduler=EPSILON_SCHEDULER_TYPE_MAP[EPSILON_SCHEDULER_TYPE](**EPSILON_SCHEDULER_CONFIG), \n",
    "        seed=SEED,\n",
    "        retriever_device=RETRIEVER_DEVICE, \n",
    "        classification_device=CLASSIFIER_DEVICE,\n",
    "        classification_tokenizer=classifier_tokenizer,\n",
    "        loss_ema_alpha=LOSS_EMA_ALPHA,\n",
    "    )\n",
    "    \n",
    "    trainer = BoostingTrainer(\n",
    "        model=classifier.to(CLASSIFIER_DEVICE),\n",
    "        args=training_args, \n",
    "        tokenizer=classifier_tokenizer, \n",
    "        train_dataset=tokenized_training, \n",
    "        eval_dataset=tokenized_validation,\n",
    "        data_collator=transformers.DataCollatorWithPadding(\n",
    "            tokenizer=classifier_tokenizer\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 25000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 40\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 120\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1045\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1045' max='1045' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1045/1045 08:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>499</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.138188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>0.189000</td>\n",
       "      <td>0.141031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25000\n",
      "  Batch size = 180\n",
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
      "/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25000\n",
      "  Batch size = 180\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n",
      "/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3079577... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600b4f119fd448f4a48fd2aaf8407fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.04MB of 0.04MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁█</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁▇▇█</td></tr><tr><td>train/global_step</td><td>▁▁▇▇█</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.14103</td></tr><tr><td>eval/runtime</td><td>23.4115</td></tr><tr><td>eval/samples_per_second</td><td>1067.851</td></tr><tr><td>eval/steps_per_second</td><td>5.937</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>1045</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0903</td></tr><tr><td>train/total_flos</td><td>3.288888192e+16</td></tr><tr><td>train/train_loss</td><td>0.13665</td></tr><tr><td>train/train_runtime</td><td>519.5656</td></tr><tr><td>train/train_samples_per_second</td><td>240.586</td></tr><tr><td>train/train_steps_per_second</td><td>2.011</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">wise-paper-20</strong>: <a href=\"https://wandb.ai/julesgm/RetroBoost/runs/2hmeup0s\" target=\"_blank\">https://wandb.ai/julesgm/RetroBoost/runs/2hmeup0s</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220210_165823-2hmeup0s/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.StupidRetriever at 0x7fdfcd28fac0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46daadc73974f0324ecc1592e5131128499dc93a3a1cbadf14a4773500af3ac4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
