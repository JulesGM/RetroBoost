{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "import sys\n",
    "\n",
    "from beartype import beartype\n",
    "import datasets\n",
    "import numpy as np\n",
    "import more_itertools\n",
    "import queue\n",
    "import rich\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import wandb\n",
    "\n",
    "TokenizerType = transformers.tokenization_utils_fast.PreTrainedTokenizerFast\n",
    "\n",
    "class BaseEpsilonScheduler(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def __call__(self):\n",
    "        pass\n",
    "\n",
    "class LinearEpsilonScheduler(BaseEpsilonScheduler):\n",
    "    def __init__(self, epsilon, num_steps):\n",
    "        self.epsilon = epsilon\n",
    "        self.num_steps = num_steps\n",
    "        self.epoch = 0\n",
    "\n",
    "    def __call__(self):\n",
    "        self.epoch += 1\n",
    "        epsilon = min(self.epsilon * (1 - self.epoch / self.num_epochs), 1)\n",
    "        wandb.log({\"epsilon\": epsilon})\n",
    "        wandb.log({\"epsilon_num_steps\": self.num_steps})\n",
    "        return epsilon\n",
    "\n",
    "class ConstantEpsilonScheduler(BaseEpsilonScheduler):\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __call__(self):\n",
    "        epsilon = self.epsilon\n",
    "        wandb.log({\"epsilon\": epsilon})\n",
    "        return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRetriever(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def retrieve(self, query_ids, query_index):\n",
    "        pass\n",
    "\n",
    "\n",
    "class StupidRetriever(BaseRetriever): \n",
    "    @beartype\n",
    "    def __init__(\n",
    "        self, *, model, tokenizer: TokenizerType, device: Union[int, str], \n",
    "        train_vectors: torch.Tensor, train_samples_dict: Dict[str, Any],\n",
    "    ):\n",
    "    \n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.train_vectors = train_vectors\n",
    "        self.train_samples_dict = train_samples_dict\n",
    "\n",
    "    def retrieve(self, query_index):\n",
    "        # Get the representation\n",
    "        representation = self.train_vectors[query_index]\n",
    "        with torch.inference_mode():\n",
    "            # Compute the inner products\n",
    "            scores = torch.matmul(representation, self.train_vectors.t())\n",
    "            # Get the top 2 results, to potentially exclude the sample itself.\n",
    "            topk = torch.topk(scores, k=2, dim=-1)\n",
    "        topk = topk.indices.cpu().numpy()\n",
    "        \n",
    "        for retrieved_idx in topk:\n",
    "            if retrieved_idx != query_index:\n",
    "                return {k: v[retrieved_idx] for k, v in self.train_samples_dict.items()} | {\"index\": retrieved_idx}\n",
    "        \n",
    "# build train vectors\n",
    "@beartype\n",
    "def make_retrival_model_and_vectors(\n",
    "    retriever_name: str, path_to_vectors: Union[str, Path], device: Union[int, str], dataset_type: str,\n",
    ") -> BaseRetriever:\n",
    "    \"\"\"We expect the dir to have the following structure:\n",
    "    - config.json\n",
    "    - train_samples.json \n",
    "    - train_vectors.npy\n",
    "    \"\"\"    \n",
    "    # Make some checks\n",
    "    retriever_model = transformers.AutoModel.from_pretrained(retriever_name)\n",
    "    retriever_tokenizer = transformers.AutoTokenizer.from_pretrained(retriever_name)\n",
    "\n",
    "    with open(path_to_vectors / \"train_samples.json\") as f:\n",
    "        train_samples_dict = json.load(f)\n",
    "        \n",
    "\n",
    "    vectors = torch.tensor(np.load(path_to_vectors / \"train_vectors.npy\")).to(device)\n",
    "    retriever = StupidRetriever(\n",
    "        model=retriever_model, \n",
    "        tokenizer=retriever_tokenizer, \n",
    "        device=device, \n",
    "        train_vectors=vectors, \n",
    "        train_samples_dict=train_samples_dict,\n",
    "    )\n",
    "    \n",
    "    return retriever\n",
    "\n",
    "\n",
    "@dataclass(order=True)\n",
    "class PrioritizedItem:\n",
    "    priority: int\n",
    "    item: Any=field(compare=False)\n",
    "\n",
    "\n",
    "class BoostingIterator(torch.utils.data.IterableDataset):\n",
    "    @beartype\n",
    "    def __init__(\n",
    "        self, \n",
    "        *, \n",
    "        dataset, \n",
    "        retriever_client: BaseRetriever, \n",
    "        classifier: nn.Module, seed: int, \n",
    "        classification_device: Union[int, str], \n",
    "        classification_tokenizer: TokenizerType, \n",
    "        retriever_device: Union[int, str],\n",
    "        epsilon_scheduler: BaseEpsilonScheduler, \n",
    "        loss_ema_alpha: float, \n",
    "        score_mode: str,\n",
    "        fixed_loss_warmup_steps: Optional[int]=None,\n",
    "        config: Dict[str, Any],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset.map(\n",
    "            lambda example, idx:{\"index\": idx}, with_indices=True, \n",
    "        ).shuffle(seed=seed)\n",
    "        self.dataset = self.dataset.remove_columns([\"idx\"])\n",
    "        self.priority_queue = queue.PriorityQueue()\n",
    "        self.retriever_client = retriever_client\n",
    "        self.epsilon_scheduler = epsilon_scheduler\n",
    "        self.randomizer = np.random.RandomState(seed)\n",
    "        self.seed = seed\n",
    "        self.dataset_iter = None\n",
    "        self.classifier = classifier\n",
    "        self.classification_tokenizer = classification_tokenizer\n",
    "        self.classification_device = classification_device\n",
    "        self.retriever_device = retriever_device\n",
    "        self.variance_rolling_average = None\n",
    "        self.loss_ema_alpha = loss_ema_alpha\n",
    "        self.dataset_type = config[\"dataset_type\"]\n",
    "        self.seen_samples = 0\n",
    "        self.epoch_length = len(self.dataset)\n",
    "        self.total_num_steps = 0\n",
    "        self.score_mode = score_mode\n",
    "        self.fixed_loss_warmup_steps = fixed_loss_warmup_steps\n",
    "        self.loss_moving_average = None\n",
    "\n",
    "        if self.score_mode == \"fixed_loss\" or self.score_mode == \"step_sensitive_fixed_loss\":\n",
    "            assert self.fixed_loss is not None\n",
    "\n",
    "\n",
    "        if self.dataset_type == \"dual_entry_classification\":\n",
    "            self.field_a_name = config[\"field_a_name\"]\n",
    "            self.field_b_name = config[\"field_b_name\"]\n",
    "\n",
    "        assert \"idx\" not in self.dataset\n",
    "\n",
    "        # assert mode in [\"epsilon_priority_no_reset\", \"pure_sampled\", \"epsilon_sampled\"], mode\n",
    "\n",
    "    def push_score(self, inputs, loss):\n",
    "        loss: Final = loss\n",
    "        inputs: Final = inputs\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            ################################################################################\n",
    "            # Moving average of the loss.\n",
    "            ################################################################################    \n",
    "            uniform_samples_loss: Final = loss[torch.logical_not(inputs[\"is_retrieved\"])].detach()\n",
    "\n",
    "            # Only use the uniform random samples to evaluate the batch's average loss.\n",
    "            # Protection against the edge case where everything is retrieved.\n",
    "            if len(uniform_samples_loss) != 0:\n",
    "                average_loss: Final = torch.mean(uniform_samples_loss).detach().cpu().numpy()\n",
    "                if self.loss_moving_average is None:\n",
    "                    self.loss_moving_average = average_loss\n",
    "                else:\n",
    "                    self.loss_moving_average = (\n",
    "                        self.loss_ema_alpha * self.loss_moving_average + (1 - self.loss_ema_alpha) * average_loss\n",
    "                    )\n",
    "                wandb.log({\"loss_moving_average\": self.loss_moving_average})\n",
    "\n",
    "            ################################################################################\n",
    "            # Scores the inputs and pushes them to the priority queue.\n",
    "            ################################################################################\n",
    "\n",
    "            for i, (input_, mask, loss_, index) in (\n",
    "                enumerate(more_itertools.zip_equal(inputs[\"input_ids\"], inputs[\"attention_mask\"], loss, inputs[\"index\"]))\n",
    "            ):\n",
    "                \n",
    "                loss_ = loss_.detach().cpu().numpy()\n",
    "                relative_loss = loss_ / self.loss_moving_average\n",
    "                \n",
    "\n",
    "                if inputs[\"has_previous_loss\"][i]:\n",
    "                    previous_loss = inputs[\"previous_loss\"][i]\n",
    "\n",
    "                    wandb.log({\"previous_relative_loss\": previous_loss})\n",
    "                    wandb.log({\"current_relative_loss\": relative_loss})\n",
    "                    wandb.log({\"ratio_relative_losses\": relative_loss / previous_loss})\n",
    "                    wandb.log({\"average_relative_losses_check\": (relative_loss + previous_loss) / 2})\n",
    "\n",
    "                assert loss_.shape == torch.Size([]), loss_.shape\n",
    "\n",
    "                if self.score_mode == \"relative_loss\":\n",
    "                    score = - relative_loss\n",
    "                elif self.score_mode == \"step_sensitive_relative_loss\":\n",
    "                    score = - relative_loss * self.total_num_steps\n",
    "                elif self.score_mode == \"fixed_loss\":\n",
    "                    score = np.abs(loss_ - self.fixed_loss)\n",
    "                elif self.score_mode == \"step_sensitive_fixed_loss\":\n",
    "                    score = np.abs(loss_ - self.fixed_loss) * self.total_num_steps \n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown score mode: {self.score_mode}\")\n",
    "\n",
    "                self.priority_queue.put(\n",
    "                    PrioritizedItem(\n",
    "                            priority=score, \n",
    "                            item=dict(input_ids=input_, attention_mask=mask, index=index, previous_loss=relative_loss)\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.dataset_iter is None:\n",
    "            self.dataset_iter = iter(self.dataset)\n",
    "        self.seen_samples = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.seen_samples == self.epoch_length:\n",
    "            raise StopIteration\n",
    "\n",
    "        # This next is only called by the training dataset.\n",
    "        self.total_num_steps += 1\n",
    "\n",
    "        # Test if we have a sample and if we pass the epsilon threshold\n",
    "        empty = self.priority_queue.empty()\n",
    "        rand = self.randomizer.rand()\n",
    "        if not empty and rand < self.epsilon_scheduler():\n",
    "            ################################################################################\n",
    "            # Retrieved sample\n",
    "            ################################################################################\n",
    "            # pull a sample from the priority queue\n",
    "            sample = self.priority_queue.get().item\n",
    "            \n",
    "            # We retrieve the next sample.\n",
    "            next_sample = self.retriever_client.retrieve(sample[\"index\"])\n",
    "            \n",
    "            \n",
    "            next_sample[\"is_retrieved\"] = True\n",
    "            next_sample[\"previous_loss\"] = sample[\"previous_loss\"]\n",
    "            next_sample[\"has_previous_loss\"] = True\n",
    "        else:\n",
    "            ################################################################################\n",
    "            # Uniform random sample\n",
    "            ################################################################################\n",
    "            try:\n",
    "                next_sample = next(self.dataset_iter)  \n",
    "            except StopIteration:\n",
    "                self.dataset = self.dataset.shuffle(seed=self.seed)\n",
    "                self.dataset_iter = iter(self.dataset)\n",
    "                next_sample = next(self.dataset_iter)  \n",
    "            next_sample[\"is_retrieved\"] = False\n",
    "            next_sample[\"previous_loss\"] = float('nan')\n",
    "            next_sample[\"has_previous_loss\"] = False\n",
    "\n",
    "        ################################################################################\n",
    "        # Per dataset type preparation\n",
    "        ################################################################################\n",
    "        if self.dataset_type == \"single_entry_classification\":\n",
    "            tokenized = self.classification_tokenizer.encode_plus(\n",
    "                next_sample[\"inputs\"], \n",
    "                truncation=True, \n",
    "                padding=True,    \n",
    "            )\n",
    "            del next_sample[\"inputs\"]\n",
    "        elif self.dataset_type == \"dual_entry_classification\":\n",
    "            tokenized = self.classification_tokenizer.encode_plus(\n",
    "                next_sample[self.field_a_name], \n",
    "                next_sample[self.field_b_name], \n",
    "                truncation=True, \n",
    "                padding=True,\n",
    "            )\n",
    "            del next_sample[self.field_a_name]\n",
    "            del next_sample[self.field_b_name]\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dataset type: {self.dataset_type}\")\n",
    "        # import html\n",
    "        # if next_sample[\"is_retrieved\"]:\n",
    "        #     wandb.log(\n",
    "        #         {\n",
    "        #         \"thing_pair\":[\n",
    "        #             wandb.Html(f\"<b>previous:</b>      {sample['index']}      {html.escape(self.classification_tokenizer.decode(sample['input_ids']   ))})\"),\n",
    "        #             wandb.Html(f\"<b>current:&nbsp;</b> {next_sample['index']} {html.escape(self.classification_tokenizer.decode(tokenized['input_ids']))})\")\n",
    "        #         ]\n",
    "        #         }\n",
    "        #        \n",
    "        #     )\n",
    "        # rich.print(\n",
    "        #     f\"[bold green]previous:[/]  {sample['index']} {self.classification_tokenizer.decode(sample['input_ids'])}\\n\", \n",
    "        #     f\"[bold green]current:[/]   {next_sample['index']} {self.classification_tokenizer.decode(tokenized['input_ids'])}\"\n",
    "        # )\n",
    "        # print(\"#\" * 80)\n",
    "\n",
    "        assert len(tokenized.keys() & next_sample.keys()) == 0, (tokenized.keys(), next_sample.keys()) \n",
    "        retval = dict(**tokenized, **next_sample)\n",
    "        assert \"previous_loss\" in retval, retval.keys()\n",
    "        return retval\n",
    "\n",
    "\n",
    "class BoostingTrainer(transformers.Trainer):\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        assert \"labels\" in inputs, inputs.keys()\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        assert labels is None\n",
    "\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        # Save past state if it exists\n",
    "        # TODO: this needs to be fixed and made cleaner later.\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            assert False\n",
    "            loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "        Subclass and override to inject custom behavior.\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "                The dictionary will be unpacked before being fed to the model. \n",
    "                Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "        Return:\n",
    "            `torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        \n",
    "        ################################################################################\n",
    "        # Remove the parts of the inputs that model.forward does not need.\n",
    "        ################################################################################\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        index = inputs[\"index\"]\n",
    "        is_retrieved = inputs[\"is_retrieved\"]\n",
    "        previous_loss = inputs[\"previous_loss\"]\n",
    "        has_previous_loss = inputs[\"has_previous_loss\"]\n",
    "        del inputs[\"previous_loss\"]\n",
    "        del inputs[\"is_retrieved\"]\n",
    "        del inputs[\"index\"]\n",
    "        del inputs[\"has_previous_loss\"]\n",
    "\n",
    "        with self.autocast_smart_context_manager():\n",
    "            # Get the loss\n",
    "            loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            # Mean over per gpu averages\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "        # This is ignored in the priority queue computation\n",
    "        if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:\n",
    "            assert False\n",
    "            # Deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "        \n",
    "        if self.do_grad_scaling:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        elif self.use_apex:\n",
    "            assert False\n",
    "            with torch.cuda.amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        elif self.deepspeed:\n",
    "            assert False\n",
    "            # loss gets scaled under gradient_accumulation_steps in deepspeed\n",
    "            loss = self.deepspeed.backward(loss)\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        loss = loss.detach()\n",
    "\n",
    "        # Addition for RetroBoost\n",
    "        # Make sure the losses are similar, then push them to the priority queue\n",
    "        # Put index back in\n",
    "\n",
    "        inputs[\"index\"] = index\n",
    "        inputs[\"is_retrieved\"] = is_retrieved\n",
    "        inputs[\"previous_loss\"] = previous_loss\n",
    "        inputs[\"has_previous_loss\"] = has_previous_loss\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            loss_per_sample = torch.nn.functional.cross_entropy(outputs.logits.detach(), inputs[\"labels\"].detach(), reduction=\"none\")\n",
    "            assert loss_per_sample.ndim == 1, loss_per_sample.ndim\n",
    "            loss_per_gpu = torch.mean(loss_per_sample, dim=0)\n",
    "            computed_loss = torch.mean(loss_per_gpu)\n",
    "            # rich.print(\"[red bold]logits[/]\", outputs.logits.detach().cpu().numpy())\n",
    "            # rich.print(\"[red bold]logits[/]\", outputs.logits.detach().cpu().numpy().shape)\n",
    "            # rich.print(\"[red bold]LOSS[/]\", loss.detach().cpu().numpy(), \" [red bold]computed_loss[/]\", computed_loss)\n",
    "            # assert torch.allclose(loss, computed_loss)\n",
    "\n",
    "            self.get_train_dataloader().dataset.push_score(inputs, loss_per_sample)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:111og4m2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 672782... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b04161c77b48699e270685e5743f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 3.95MB of 3.95MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_relative_losses_check</td><td>▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▁▂▃▁▂▃▁▇▂▃▂▂▁▄▃▃▂▃█▅▁</td></tr><tr><td>current_relative_loss</td><td>▃▃▃▃▃▃▃▃▃▃▃▄▃▄▃▃▄▃▃▂▃▃▁▄▅▂▅▂▁▂▂▂▁▄▁▂▁█▁▁</td></tr><tr><td>epsilon</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/accuracy</td><td>▂▂▁▁▁▁▁▁▄▂▃▃▂▄▅▃▆▅▆▅▅▆▆▅▅▇▅▇▆▇▆▇▆█▇▇▇▇██</td></tr><tr><td>eval/loss</td><td>█████████████▇▇█▇▇▅▆▆▅▅▅▇▄▆▃▄▂▅▂▄▂▂▃▃▃▁▂</td></tr><tr><td>eval/runtime</td><td>█▂▂▁▂▂▁▂▁▂▂▂▁▂▂▂▂▁▂▂▂▃▁▂▁▂▁▁▁▂▂▁▂▁▂▂▁▁▂▃</td></tr><tr><td>eval/samples_per_second</td><td>▁▇▇█▇▇▇▇█▇▇▇█▇▇▇▇█▇▇▇▆▇▇█▇▇██▇▇▇▇█▇▆█▇▇▆</td></tr><tr><td>eval/steps_per_second</td><td>▁▇▇█▇▇▇▇█▇▇▇█▇▇▇▇█▇▇▇▆▇▇█▇▇██▇▇▇▇█▇▆█▇▇▆</td></tr><tr><td>loss_moving_average</td><td>██████████████████▇▇▇▇▆▆▆▆▆▆▅▅▅▄▄▄▃▃▂▂▂▁</td></tr><tr><td>previous_relative_loss</td><td>▂▁▁▁▂▂▁▁▂▂▁▂▁▃▂▂▁▂▂▂▂▃▁▁▁▁█▂▅▂▂▁▆▁▄▃▅▇█▂</td></tr><tr><td>ratio_relative_losses</td><td>▄▄▄▅▄▃▅▄▄▄▅▄▄▃▄▄▅▃▃▂▃▃▁▅█▃▂▃▁▂▂▂▁▆▁▁▁▄▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_relative_losses_check</td><td>0.35335</td></tr><tr><td>current_relative_loss</td><td>0.15095</td></tr><tr><td>epsilon</td><td>0.5</td></tr><tr><td>eval/accuracy</td><td>0.85561</td></tr><tr><td>eval/loss</td><td>0.43105</td></tr><tr><td>eval/runtime</td><td>0.7268</td></tr><tr><td>eval/samples_per_second</td><td>514.55</td></tr><tr><td>eval/steps_per_second</td><td>17.885</td></tr><tr><td>loss_moving_average</td><td>0.49331</td></tr><tr><td>previous_relative_loss</td><td>0.55576</td></tr><tr><td>ratio_relative_losses</td><td>0.2716</td></tr><tr><td>train/epoch</td><td>4.72</td></tr><tr><td>train/global_step</td><td>500</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.5668</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fp16_step_sensitive_relative_loss_LOSS_EMA_ALPHA=0.995_constant_epsilon</strong>: <a href=\"https://wandb.ai/retroboost/RetroBoost/runs/111og4m2\" target=\"_blank\">https://wandb.ai/retroboost/RetroBoost/runs/111og4m2</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220213_004853-111og4m2/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:111og4m2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/retroboost/RetroBoost/runs/1ujfg5d8\" target=\"_blank\">fp16_step_sensitive_relative_loss_LOSS_EMA_ALPHA=0.995_constant_epsilon</a></strong> to <a href=\"https://wandb.ai/retroboost/RetroBoost\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Reusing dataset super_glue (/home/mila/g/gagnonju/.cache/huggingface/datasets/super_glue/rte/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n",
      "Reusing dataset super_glue (/home/mila/g/gagnonju/.cache/huggingface/datasets/super_glue/rte/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8567779869234997a7ea938558184fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a65914d1b6475aba28e44284da8919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "loading configuration file https://huggingface.co/facebook/contriever/resolve/main/config.json from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/52956acf642ee38953ffe7ea253f16896223c887093b1cbbdcec52c82ed6ea6c.b19e5a240d78d865c8542d06f7a66ec533465c843c39fa863e76ebb42cd7a581\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"facebook/contriever\",\n",
      "  \"architectures\": [\n",
      "    \"Contriever\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/facebook/contriever/resolve/main/pytorch_model.bin from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/8536bee11f14ad7063b2489c148f67a958ab83ab3b727ae6a1846a6eb7d86ba4.b0886cf70b0752007a1d3a6161622704be1f74c942b77bdfec51570558c35264\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at facebook/contriever.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file https://huggingface.co/facebook/contriever/resolve/main/vocab.txt from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/be0b434deb0e60a3e1dbef9cd5a007ab66ccc34a6caaf37bb261ad28663449a1.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/facebook/contriever/resolve/main/tokenizer.json from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/46f1d52bf62c9485839492be3ddff22058e3d76592e1c6a338e28061c6c711bd.f471bd2d72c48b932f7be40446896b7e97c3be406ee93abfb500399bc606c829\n",
      "loading file https://huggingface.co/facebook/contriever/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/facebook/contriever/resolve/main/special_tokens_map.json from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/ac7a4b49e74445ae37291a5057a526b64b9a80a70ffa9d27dc756980bfa9bb1b.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/facebook/contriever/resolve/main/tokenizer_config.json from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/786ef1d235d43b5473c1bc5e6ac3c98f8603ef39456124f54a1df2718a0cb2e8.59407384618422b5f582b6046df91db98a0f921d6c959dc7b1f50000ffea1032\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATASET_TUPLE = (\"super_glue\", \"rte\")\n",
    "CLASSIFIER_NAME = \"roberta-base\"\n",
    "CLASSIFIER_BATCH_SIZE = 20\n",
    "EPSILON_SCHEDULER_TYPE = \"constant\"\n",
    "EPSILON_SCHEDULER_CONFIG = dict(\n",
    "    epsilon=.5,\n",
    ")\n",
    "LOSS_EMA_ALPHA = 0.995\n",
    "REGULAR_TRAINER = False\n",
    "WEIGHT_DECAY = 0.01\n",
    "LEARNING_RATE = 1e-5\n",
    "ENABLE_FP16 = True\n",
    "\n",
    "SCORE_MODE = \"step_sensitive_relative_loss\"\n",
    "FIXED_LOSS_WARMUP_STEPS = 10\n",
    "\n",
    "\n",
    "# Things that don't change\n",
    "RETRIEVER_NAME = \"facebook/contriever\"\n",
    "PATH_TO_VECTORS = Path(f\"./vectors_{'_'.join(DATASET_TUPLE)}_{RETRIEVER_NAME.split('/')[-1]}/\")\n",
    "CLASSIFIER_EVAL_BATCH_SIZE_MULTIPLIER = 1.5\n",
    "CLASSIFIER_DEVICE = \"cuda\"\n",
    "RETRIEVER_DEVICE = \"cuda\"\n",
    "SEED = 0\n",
    "SPLIT_RATIO = 0.85\n",
    "NUM_EPOCHS_TO_TRAIN_ON = 60\n",
    "\n",
    "\n",
    "\n",
    "RUN_NAME = f\"{'fp16' if ENABLE_FP16 else 'fp32'}_{SCORE_MODE}_{LOSS_EMA_ALPHA=}_{EPSILON_SCHEDULER_TYPE}_epsilon\"\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Fast setup \n",
    "###############################################################################\n",
    "config: Final = json.loads((PATH_TO_VECTORS / \"config.json\").read_text())\n",
    "assert config[\"retriever_name\"] == RETRIEVER_NAME, f\"{config['retriever_name']} != {RETRIEVER_NAME}\"\n",
    "\n",
    "wandb_config = dict(\n",
    "        classifier_batch_size=CLASSIFIER_BATCH_SIZE,\n",
    "        classifier_name=CLASSIFIER_NAME,\n",
    "        dataset_tuple=DATASET_TUPLE,\n",
    "        epsilon=dict(\n",
    "            scheduler_type=EPSILON_SCHEDULER_TYPE,\n",
    "            scheduler_config=EPSILON_SCHEDULER_CONFIG,\n",
    "        ),\n",
    "        loss_ema_alpha=LOSS_EMA_ALPHA,\n",
    "        random_seed=SEED,\n",
    "        regular_trainer=REGULAR_TRAINER,\n",
    "        retriever_name=RETRIEVER_NAME,\n",
    "        split_ratio=SPLIT_RATIO,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        dataset_type=config[\"dataset_type\"],\n",
    "        enable_fp16=ENABLE_FP16,\n",
    "    )\n",
    "\n",
    "wandb.init(\n",
    "    config=wandb_config,\n",
    "    project=\"RetroBoost\", \n",
    "    entity=\"retroboost\",\n",
    "    name=RUN_NAME,\n",
    ")\n",
    "\n",
    "EPSILON_SCHEDULER_TYPE_MAP = dict(\n",
    "    constant=ConstantEpsilonScheduler,\n",
    ")\n",
    "\n",
    "# Random seeds. \n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "classifier_tokenizer: Final = transformers.AutoTokenizer.from_pretrained(CLASSIFIER_NAME)\n",
    "\n",
    "# Load the config\n",
    "\n",
    "# Load the datasets\n",
    "dataset_train: Final = datasets.load_dataset(*DATASET_TUPLE, split=f\"train[:{SPLIT_RATIO:.0%}]\")\n",
    "dataset_validation: Final = datasets.load_dataset(*DATASET_TUPLE, split=f\"train[{SPLIT_RATIO:.0%}:]\")\n",
    "\n",
    "ALL_LABELS = set(dataset_train[\"label\"])\n",
    "NUM_LABELS = len(ALL_LABELS)\n",
    "assert ALL_LABELS == set(range(NUM_LABELS))\n",
    "\n",
    "# Delete the extra fields\n",
    "if config[\"dataset_type\"] == \"dual_entry_classification\":\n",
    "    fields = dataset_train[0].keys()\n",
    "    dataset_train.remove_columns(fields - {config[\"field_a_name\"], config[\"field_b_name\"], \"label\"} )\n",
    "\n",
    "def preprocess_function(examples, tokenizer, config):\n",
    "    if config[\"dataset_type\"] == \"single_entry_classification\":\n",
    "        return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "    elif config[\"dataset_type\"] == \"dual_entry_classification\":\n",
    "        return tokenizer(\n",
    "            examples[config[\"field_a_name\"]], \n",
    "            examples[config[\"field_b_name\"]], \n",
    "            truncation=True, \n",
    "            padding=True,\n",
    "        )\n",
    "\n",
    "    raise ValueError(f\"Unknown dataset type {config['dataset_type']}\")\n",
    "\n",
    "tokenized_training: Final = dataset_train.map(\n",
    "    lambda examples: preprocess_function(examples, classifier_tokenizer, config), \n",
    "    batched=True\n",
    ").shuffle(seed=SEED)\n",
    "\n",
    "tokenized_validation: Final = dataset_validation.map(\n",
    "    lambda examples: preprocess_function(examples, classifier_tokenizer, config), \n",
    "    batched=True\n",
    ").shuffle(seed=SEED)\n",
    "\n",
    "training_args: Final = transformers.TrainingArguments(\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=CLASSIFIER_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=int(CLASSIFIER_BATCH_SIZE * CLASSIFIER_EVAL_BATCH_SIZE_MULTIPLIER),\n",
    "    num_train_epochs=NUM_EPOCHS_TO_TRAIN_ON,\n",
    "    report_to=\"wandb\",\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    fp16=ENABLE_FP16,\n",
    ")\n",
    "\n",
    "retriever: Final = make_retrival_model_and_vectors(\n",
    "    retriever_name=RETRIEVER_NAME, \n",
    "    path_to_vectors=PATH_TO_VECTORS, \n",
    "    device=RETRIEVER_DEVICE, \n",
    "    dataset_type=config[\"dataset_type\"],\n",
    ")\n",
    "retriever_client: Final = retriever\n",
    "\n",
    "classifier: Final = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    CLASSIFIER_NAME, num_labels=NUM_LABELS\n",
    ")\n",
    "\n",
    "classifier.config.problem_type = \"single_label_classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return datasets.load_metric(\"accuracy\").compute(predictions=predictions, references=labels)\n",
    "\n",
    "if REGULAR_TRAINER:\n",
    "    TrainerClass = transformers.Trainer    \n",
    "    ds_train = tokenized_training\n",
    "else:\n",
    "    TrainerClass = BoostingTrainer\n",
    "    \n",
    "    ds_train = BoostingIterator(\n",
    "        dataset=dataset_train, \n",
    "        retriever_client=retriever_client, \n",
    "        classifier=classifier, \n",
    "        epsilon_scheduler=EPSILON_SCHEDULER_TYPE_MAP[EPSILON_SCHEDULER_TYPE](**EPSILON_SCHEDULER_CONFIG), \n",
    "        seed=SEED,\n",
    "        retriever_device=RETRIEVER_DEVICE, \n",
    "        classification_device=CLASSIFIER_DEVICE,\n",
    "        classification_tokenizer=classifier_tokenizer,\n",
    "        loss_ema_alpha=LOSS_EMA_ALPHA,\n",
    "        config=config,\n",
    "        score_mode=SCORE_MODE,\n",
    "        fixed_loss_warmup_steps=FIXED_LOSS_WARMUP_STEPS,   \n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorWithPadding:\n",
    "    tokenizer: transformers.data.data_collator.PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, transformers.data.data_collator.PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        # check that they all have the same keys\n",
    "        all_keys = set()\n",
    "        for feature in features:\n",
    "            all_keys |= feature.keys()\n",
    "        \n",
    "        for feature in features:\n",
    "            assert all_keys == feature.keys(), all_keys - feature.keys()\n",
    "\n",
    "        first = features[0]\n",
    "        \n",
    "\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "\n",
    "        if \"label\" in batch:\n",
    "            batch[\"labels\"] = batch[\"label\"]\n",
    "            del batch[\"label\"]\n",
    "        \n",
    "        if \"label_ids\" in batch:\n",
    "            batch[\"labels\"] = batch[\"label_ids\"]\n",
    "            del batch[\"label_ids\"]\n",
    "        \n",
    "        return batch\n",
    "\n",
    "\n",
    "trainer = TrainerClass(\n",
    "        model=classifier,\n",
    "        args=training_args, \n",
    "        tokenizer=classifier_tokenizer, \n",
    "        train_dataset=ds_train, \n",
    "        eval_dataset=tokenized_validation,\n",
    "        data_collator=transformers.data.data_collator.DataCollatorWithPadding(\n",
    "            tokenizer=classifier_tokenizer\n",
    "        ),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2116\n",
      "  Num Epochs = 60\n",
      "  Instantaneous batch size per device = 20\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 20\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6360\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1211' max='6360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1211/6360 03:48 < 16:13, 5.29 it/s, Epoch 11.42/60]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693000</td>\n",
       "      <td>0.505348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692936</td>\n",
       "      <td>0.540107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692846</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692905</td>\n",
       "      <td>0.505348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692771</td>\n",
       "      <td>0.505348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692670</td>\n",
       "      <td>0.505348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692516</td>\n",
       "      <td>0.505348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692666</td>\n",
       "      <td>0.505348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693361</td>\n",
       "      <td>0.505348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693566</td>\n",
       "      <td>0.505348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.694050</td>\n",
       "      <td>0.505348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693904</td>\n",
       "      <td>0.505348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.690347</td>\n",
       "      <td>0.508021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.684863</td>\n",
       "      <td>0.622995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.673540</td>\n",
       "      <td>0.558824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.658737</td>\n",
       "      <td>0.647059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.680473</td>\n",
       "      <td>0.663102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.607594</td>\n",
       "      <td>0.692513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.639740</td>\n",
       "      <td>0.703209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.637287</td>\n",
       "      <td>0.606952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.607633</td>\n",
       "      <td>0.713904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.580146</td>\n",
       "      <td>0.703209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.568736</td>\n",
       "      <td>0.743316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.689044</td>\n",
       "      <td>0.708556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.576349</td>\n",
       "      <td>0.721925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.551407</td>\n",
       "      <td>0.711230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.611095</td>\n",
       "      <td>0.689840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.575581</td>\n",
       "      <td>0.719251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.533251</td>\n",
       "      <td>0.740642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.539798</td>\n",
       "      <td>0.743316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.557732</td>\n",
       "      <td>0.743316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.549575</td>\n",
       "      <td>0.745989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.621122</td>\n",
       "      <td>0.721925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.679766</td>\n",
       "      <td>0.721925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.655768</td>\n",
       "      <td>0.732620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.601375</td>\n",
       "      <td>0.754011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.565592</td>\n",
       "      <td>0.756684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.673955</td>\n",
       "      <td>0.745989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.574298</td>\n",
       "      <td>0.754011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.539695</td>\n",
       "      <td>0.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.638625</td>\n",
       "      <td>0.754011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.587582</td>\n",
       "      <td>0.767380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.709343</td>\n",
       "      <td>0.756684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.708119</td>\n",
       "      <td>0.759358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.679771</td>\n",
       "      <td>0.762032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.699025</td>\n",
       "      <td>0.748663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.673398</td>\n",
       "      <td>0.762032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.733552</td>\n",
       "      <td>0.759358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.606026</td>\n",
       "      <td>0.783422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.670701</td>\n",
       "      <td>0.754011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.657595</td>\n",
       "      <td>0.754011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.671826</td>\n",
       "      <td>0.780749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.840652</td>\n",
       "      <td>0.754011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.845665</td>\n",
       "      <td>0.756684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.784185</td>\n",
       "      <td>0.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.762032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.768140</td>\n",
       "      <td>0.786096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.736806</td>\n",
       "      <td>0.788770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.933188</td>\n",
       "      <td>0.743316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.740950</td>\n",
       "      <td>0.775401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.899706</td>\n",
       "      <td>0.762032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.904548</td>\n",
       "      <td>0.775401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.950660</td>\n",
       "      <td>0.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.829895</td>\n",
       "      <td>0.778075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.789483</td>\n",
       "      <td>0.770053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.029220</td>\n",
       "      <td>0.762032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.920561</td>\n",
       "      <td>0.783422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.051820</td>\n",
       "      <td>0.780749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.992368</td>\n",
       "      <td>0.780749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.034445</td>\n",
       "      <td>0.772727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.041481</td>\n",
       "      <td>0.762032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.965132</td>\n",
       "      <td>0.775401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.268845</td>\n",
       "      <td>0.737968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.937461</td>\n",
       "      <td>0.770053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.124734</td>\n",
       "      <td>0.767380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.060291</td>\n",
       "      <td>0.767380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.121645</td>\n",
       "      <td>0.759358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.311268</td>\n",
       "      <td>0.756684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.209352</td>\n",
       "      <td>0.772727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.325356</td>\n",
       "      <td>0.770053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.197630</td>\n",
       "      <td>0.778075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.229965</td>\n",
       "      <td>0.786096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.283432</td>\n",
       "      <td>0.775401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.335216</td>\n",
       "      <td>0.759358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.203428</td>\n",
       "      <td>0.772727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.286786</td>\n",
       "      <td>0.770053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.309730</td>\n",
       "      <td>0.762032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.327572</td>\n",
       "      <td>0.770053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.380432</td>\n",
       "      <td>0.767380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.434943</td>\n",
       "      <td>0.754011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.383486</td>\n",
       "      <td>0.772727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.380750</td>\n",
       "      <td>0.772727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.213365</td>\n",
       "      <td>0.783422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.296910</td>\n",
       "      <td>0.783422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.298530</td>\n",
       "      <td>0.780749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.417938</td>\n",
       "      <td>0.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.392100</td>\n",
       "      <td>0.778075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.478002</td>\n",
       "      <td>0.762032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>1.488667</td>\n",
       "      <td>0.767380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>1.482996</td>\n",
       "      <td>0.778075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>1.425153</td>\n",
       "      <td>0.775401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>1.439210</td>\n",
       "      <td>0.772727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>1.419896</td>\n",
       "      <td>0.780749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>1.443244</td>\n",
       "      <td>0.767380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>1.239969</td>\n",
       "      <td>0.756684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>1.249998</td>\n",
       "      <td>0.767380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>1.405728</td>\n",
       "      <td>0.772727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>1.545219</td>\n",
       "      <td>0.767380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>1.544765</td>\n",
       "      <td>0.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>1.584083</td>\n",
       "      <td>0.767380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>1.582165</td>\n",
       "      <td>0.756684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>1.767159</td>\n",
       "      <td>0.740642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>1.543133</td>\n",
       "      <td>0.767380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>1.719254</td>\n",
       "      <td>0.759358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>1.527824</td>\n",
       "      <td>0.775401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>1.564785</td>\n",
       "      <td>0.756684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>1.580014</td>\n",
       "      <td>0.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>1.487027</td>\n",
       "      <td>0.772727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>1.532703</td>\n",
       "      <td>0.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>1.640774</td>\n",
       "      <td>0.754011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3/13 00:00 < 00:00, 22.22 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, premise, hypothesis.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 374\n",
      "  Batch size = 30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_671356/3349114407.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.main/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1440\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1441\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.main/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1563\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.main/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2207\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2208\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   2209\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2210\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.main/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2390\u001b[0m                 \u001b[0mlosses_host\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlosses_host\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2392\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_across_processes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2393\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2394\u001b[0m                 \u001b[0mpreds_host\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreds_host\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.main/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_pad_across_processes\u001b[0;34m(self, tensor, pad_index)\u001b[0m\n\u001b[1;32m   2504\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2505\u001b[0m         \u001b[0;31m# Gather all sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2506\u001b[0;31m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2507\u001b[0m         \u001b[0msizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output = trainer.train()\n",
    "print(output)\n",
    "print(trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46daadc73974f0324ecc1592e5131128499dc93a3a1cbadf14a4773500af3ac4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
