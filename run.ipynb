{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import more_itertools\n",
    "import queue\n",
    "import rich\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import wandb\n",
    "\n",
    "\n",
    "class LinearEpsilonScheduler:\n",
    "    def __init__(self, epsilon, num_steps):\n",
    "        self.epsilon = epsilon\n",
    "        self.num_epochs = num_steps\n",
    "        self.epoch = 0\n",
    "\n",
    "    def __call__(self):\n",
    "        self.epoch += 1\n",
    "        epsilon = min(self.epsilon * (1 - self.epoch / self.num_epochs), 1)\n",
    "        wandb.log({\"epsilon\": epsilon})\n",
    "        return epsilon\n",
    "\n",
    "class ConstantEpsilonScheduler:\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __call__(self):\n",
    "        epsilon = self.epsilon\n",
    "        wandb.log({\"epsilon\": epsilon})\n",
    "        return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class StupidRetriever: \n",
    "    def __init__(self, *, model, tokenizer, device, train_vectors, train_samples, train_labels):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.train_vectors = train_vectors\n",
    "        self.train_samples = train_samples\n",
    "        self.train_labels = train_labels\n",
    "        self.classification_ids_to_idx = {}\n",
    "\n",
    "        for i, sample in enumerate(train_samples):\n",
    "            encoded = tokenizer.encode(sample, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "            self.classification_ids_to_idx[encoded] = i\n",
    "\n",
    "    def retrieve(self, query_ids, query_index):\n",
    "        # Get the representation\n",
    "        representation = self.train_vectors[query_index]\n",
    "        with torch.inference_mode():\n",
    "            # Compute the inner products\n",
    "            scores = torch.matmul(representation, self.train_vectors.t())\n",
    "        # Get the top 2 results, to potentially exclude the sample itself.\n",
    "        topk = torch.topk(scores, k=2, dim=-1)\n",
    "        topk = topk.indices.cpu().numpy()\n",
    "        \n",
    "        for retrieved_idx in topk:\n",
    "            if retrieved_idx != query_index:\n",
    "                return self.train_samples[retrieved_idx], self.train_labels[retrieved_idx], retrieved_idx\n",
    "        \n",
    "\n",
    "# build train vectors\n",
    "def make_retrival_model_and_vectors(retriever_name, path_to_vectors, device, dataset_name):\n",
    "    \"\"\"We expect the dir to have the following structure:\n",
    "    - config.json\n",
    "    - train_samples.json \n",
    "    - train_vectors.npy\n",
    "    \"\"\"    \n",
    "    # Make some checks\n",
    "    config =  json.loads((path_to_vectors / \"config.json\").read_text())\n",
    "    assert dataset_name == config[\"dataset_name\"], (dataset_name, config[\"dataset_name\"])\n",
    "    assert retriever_name == config[\"retriever_name\"], (retriever_name, config[\"retriever_name\"])\n",
    "\n",
    "    retriever_model = transformers.AutoModel.from_pretrained(retriever_name)\n",
    "    retriever_tokenizer = transformers.AutoTokenizer.from_pretrained(retriever_name)\n",
    "\n",
    "    with open(path_to_vectors / \"train_samples.json\") as f:\n",
    "        train_samples = json.load(f)\n",
    "    vectors = torch.tensor(np.load(path_to_vectors / \"train_vectors.npy\")).to(device)\n",
    "    retriever = StupidRetriever(\n",
    "        model=retriever_model, \n",
    "        tokenizer=retriever_tokenizer, \n",
    "        device=device, \n",
    "        train_vectors=vectors, \n",
    "        train_samples=train_samples[\"inputs\"],\n",
    "        train_labels=train_samples[\"labels\"],\n",
    "    )\n",
    "    \n",
    "    return retriever\n",
    "\n",
    "\n",
    "@dataclass(order=True)\n",
    "class PrioritizedItem:\n",
    "    priority: int\n",
    "    item: Any=field(compare=False)\n",
    "\n",
    "\n",
    "class BoostingIterator(torch.utils.data.IterableDataset):\n",
    "    def __init__(\n",
    "        self, *args, dataset, retriever_client, classifier, seed, \n",
    "        classification_device, classification_tokenizer, retriever_device, \n",
    "        epsilon_scheduler, **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.dataset = dataset.map(\n",
    "            lambda example, idx:{\"index\": idx}, with_indices=True\n",
    "        ).shuffle(seed=seed)\n",
    "        self.priority_queue = queue.PriorityQueue()\n",
    "        self.retriever_client = retriever_client\n",
    "        self.epsilon_scheduler = epsilon_scheduler\n",
    "        self.randomizer = np.random.RandomState(seed)\n",
    "        self.seed = seed\n",
    "        self.dataset_iter = None\n",
    "        self.classifier = classifier\n",
    "        self.classification_tokenizer = classification_tokenizer\n",
    "        self.classification_device = classification_device\n",
    "        self.retriever_device = retriever_device\n",
    "        \n",
    "        # assert mode in [\"epsilon_priority_no_reset\", \"pure_sampled\", \"epsilon_sampled\"], mode\n",
    "\n",
    "    def push_score(self, inputs, loss):\n",
    "        for input_, mask, loss_, index in (\n",
    "            more_itertools.zip_equal(inputs[\"input_ids\"], inputs[\"attention_mask\"], loss, inputs[\"index\"])\n",
    "        ):\n",
    "            assert loss_.shape == torch.Size([]), loss_.shape\n",
    "            self.priority_queue.put(\n",
    "                PrioritizedItem(\n",
    "                    priority=-loss_.detach().cpu().numpy(), \n",
    "                    item=dict(input_ids=input_, attention_mask=mask, index=index)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        rich.print(\"[bold green]ITER[/]\")\n",
    "        self.dataset = self.dataset.shuffle(seed=self.seed)\n",
    "        self.dataset_iter = iter(self.dataset)\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        \"\"\" This is where the sampling happens.\n",
    "        \"\"\"\n",
    "\n",
    "        # Test if we have a sample and if we pass the epsilon threshold\n",
    "        empty = self.priority_queue.empty()\n",
    "        rand = self.randomizer.rand()\n",
    "        if not empty and rand < self.epsilon_scheduler():\n",
    "            # pull a sample from the priority queue\n",
    "            sample = self.priority_queue.get().item\n",
    "            ids = sample[\"input_ids\"]\n",
    "            entry_indexes = sample[\"index\"]\n",
    "\n",
    "            # We retrieve the next sample.\n",
    "            input_, next_label, index = self.retriever_client.retrieve(ids, entry_indexes)\n",
    "            next_sample = dict(text=input_, label=next_label, index=index)\n",
    "        else:\n",
    "            next_sample = next(self.dataset_iter)  # We raise here if we have no more samples in the dataset\n",
    "\n",
    "        tokenized = self.classification_tokenizer(\n",
    "            next_sample[\"text\"].strip(), \n",
    "            truncation=True, \n",
    "            padding=True,\n",
    "        )\n",
    "\n",
    "        del next_sample[\"text\"]\n",
    "        return dict(**tokenized, **next_sample)\n",
    "\n",
    "class BoostingTrainer(transformers.Trainer):\n",
    "    \n",
    "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "        Subclass and override to inject custom behavior.\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "                The dictionary will be unpacked before being fed to the model. \n",
    "                Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "        Return:\n",
    "            `torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        index = inputs[\"index\"]\n",
    "        # Compute loss doesn't like this.\n",
    "        del inputs[\"index\"]\n",
    "\n",
    "        with self.autocast_smart_context_manager():\n",
    "            # Get the loss\n",
    "            loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            # Mean over per gpu averages\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "        # This is ignored in the priority queue computation\n",
    "        if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:\n",
    "            # Deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "        if self.do_grad_scaling:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        elif self.use_apex:\n",
    "            with torch.cuda.amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        elif self.deepspeed:\n",
    "            # loss gets scaled under gradient_accumulation_steps in deepspeed\n",
    "            loss = self.deepspeed.backward(loss)\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        loss = loss.detach()\n",
    "\n",
    "        # Addition for RetroBoost\n",
    "        # Make sure the losses are similar, then push them to the priority queue\n",
    "        # Put index back in\n",
    "        inputs[\"index\"] = index\n",
    "        computed_loss = torch.nn.functional.cross_entropy(outputs.logits.detach(), inputs[\"labels\"].detach(), reduction=\"mean\")\n",
    "        loss_per_sample = torch.nn.functional.cross_entropy(outputs.logits.detach(), inputs[\"labels\"].detach(), reduction=\"none\")\n",
    "        assert torch.allclose(loss, computed_loss, atol=0.1)\n",
    "        self.get_train_dataloader().dataset.push_score(inputs, loss_per_sample)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1en9jbpb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2929276... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1en9jbpb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/julesgm/RetroBoost/runs/2hmzjcqj\" target=\"_blank\">rich-fog-12</a></strong> to <a href=\"https://wandb.ai/julesgm/RetroBoost\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/mila/g/gagnonju/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bfe1be18ef4b6dab4bb65e9f53ac10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Reusing dataset imdb (/home/mila/g/gagnonju/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc7314a390d40d2ae5a975d24188f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d223a7a40748198737d525958be8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b83c54ad1e44f25a7666cfb91327c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RETRIEVER_NAME = \"facebook/contriever\"\n",
    "PATH_TO_VECTORS = Path(\"./vectors_imdb_contriever/\")\n",
    "DATASET_NAME = \"imdb\"\n",
    "CLASSIFIER_NAME = \"roberta-base\"\n",
    "EPSILON_SCHEDULER_TYPE = \"constant\"\n",
    "EPSILON_SCHEDULER_CONFIG = dict(\n",
    "    epsilon=0.9,\n",
    ")\n",
    "\n",
    "REGULAR_TRAINER = False\n",
    "CLASSIFIER_DEVICE = 1\n",
    "RETRIEVER_DEVICE = 2\n",
    "SEED = 0\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Fast setup \n",
    "###############################################################################\n",
    "config = dict(\n",
    "        retriever_name=RETRIEVER_NAME,\n",
    "        dataset_name=DATASET_NAME,\n",
    "        classifier_name=CLASSIFIER_NAME,\n",
    "        regular_trainer=REGULAR_TRAINER,\n",
    "        epsilon=dict(\n",
    "            scheduler_type=EPSILON_SCHEDULER_TYPE,\n",
    "            scheduler_config=EPSILON_SCHEDULER_CONFIG,\n",
    "        )\n",
    "    )\n",
    "\n",
    "wandb.init(\n",
    "    config=config,\n",
    "    project=\"RetroBoost\", \n",
    "    entity=\"julesgm\",\n",
    ")\n",
    "\n",
    "EPSILON_SCHEDULER_TYPE_MAP = dict(\n",
    "    constant=ConstantEpsilonScheduler,\n",
    ")\n",
    "\n",
    "# Random seeds. \n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "dataset = datasets.load_dataset(DATASET_NAME)\n",
    "ALL_LABELS = set(dataset[\"train\"][\"label\"])\n",
    "NUM_LABELS = len(ALL_LABELS)\n",
    "assert ALL_LABELS == set(range(NUM_LABELS))\n",
    "\n",
    "classifier_name = CLASSIFIER_NAME\n",
    "dataset_name = DATASET_NAME\n",
    "regular_trainer = REGULAR_TRAINER\n",
    "\n",
    "\n",
    "classifier = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    classifier_name, num_labels=NUM_LABELS\n",
    ")\n",
    "classifier_tokenizer = transformers.AutoTokenizer.from_pretrained(classifier_name)\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "dataset = datasets.load_dataset(dataset_name)\n",
    "tokenized_training = dataset[\"train\"].map(\n",
    "    lambda examples: preprocess_function(examples, classifier_tokenizer), \n",
    "    batched=True\n",
    ")\n",
    "\n",
    "tokenized_validation = dataset[\"test\"].map(\n",
    "    lambda examples: preprocess_function(examples, classifier_tokenizer), \n",
    "    batched=True\n",
    ")\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=20,\n",
    "    per_device_eval_batch_size=20,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/mila/g/gagnonju/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-a645737e8e3ed793.arrow\n",
      "Loading cached shuffled indices for dataset at /home/mila/g/gagnonju/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-42092c50c935d800.arrow\n"
     ]
    }
   ],
   "source": [
    "retriever = make_retrival_model_and_vectors(\n",
    "    retriever_name=RETRIEVER_NAME, \n",
    "    path_to_vectors=PATH_TO_VECTORS, \n",
    "    device=RETRIEVER_DEVICE, \n",
    "    dataset_name=DATASET_NAME,\n",
    ")\n",
    "retriever_client = retriever\n",
    "\n",
    "if regular_trainer:\n",
    "    trainer = transformers.Trainer(\n",
    "        model=classifier.to(CLASSIFIER_DEVICE), \n",
    "        args=training_args, \n",
    "        tokenizer=classifier_tokenizer, \n",
    "        train_dataset=tokenized_training, \n",
    "        eval_dataset=tokenized_validation,\n",
    "        data_collator=transformers.DataCollatorWithPadding(\n",
    "            tokenizer=classifier_tokenizer\n",
    "        ),\n",
    "    )\n",
    "else:\n",
    "    tokenized_training = BoostingIterator(\n",
    "        dataset=dataset[\"train\"], \n",
    "        retriever_client=retriever_client, \n",
    "        classifier=classifier, \n",
    "        epsilon_scheduler=EPSILON_SCHEDULER_TYPE_MAP[EPSILON_SCHEDULER_TYPE](**EPSILON_SCHEDULER_CONFIG), \n",
    "        seed=SEED,\n",
    "        retriever_device=RETRIEVER_DEVICE, \n",
    "        classification_device=CLASSIFIER_DEVICE,\n",
    "        classification_tokenizer=classifier_tokenizer,\n",
    "    )\n",
    "    \n",
    "    trainer = BoostingTrainer(\n",
    "        model=classifier.to(CLASSIFIER_DEVICE),\n",
    "        args=training_args, \n",
    "        tokenizer=classifier_tokenizer, \n",
    "        train_dataset=tokenized_training, \n",
    "        eval_dataset=tokenized_validation,\n",
    "        data_collator=transformers.DataCollatorWithPadding(\n",
    "            tokenizer=classifier_tokenizer\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 25000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 20\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 60\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6250\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">ITER</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mITER\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/mila/g/gagnonju/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-4dc2830ea21e6a12.arrow\n",
      "/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2868' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2868/6250 07:02 < 08:18, 6.79 it/s, Epoch 2.06/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.266600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.180900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.133200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.133500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
      "/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n",
      "/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">ITER</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mITER\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/mila/g/gagnonju/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-1172bcf5f8186130.arrow\n",
      "Saving model checkpoint to ./results/checkpoint-1500\n",
      "Configuration saved in ./results/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1500/special_tokens_map.json\n",
      "/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./results/checkpoint-2000\n",
      "Configuration saved in ./results/checkpoint-2000/config.json\n",
      "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2000/special_tokens_map.json\n",
      "/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./results/checkpoint-2500\n",
      "Configuration saved in ./results/checkpoint-2500/config.json\n",
      "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2500/special_tokens_map.json\n",
      "/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">ITER</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mITER\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/mila/g/gagnonju/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-f71e4df4d8809d6e.arrow\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2928161/3431184757.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.main/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1406\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m                             \u001b[0;31m# Revert to normal clipping otherwise, handling Apex or full precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1408\u001b[0;31m                             nn.utils.clip_grad_norm_(\n\u001b[0m\u001b[1;32m   1409\u001b[0m                                 \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_apex\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m                                 \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.main/lib/python3.9/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mclip_coef_clamped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef_clamped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output = trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46daadc73974f0324ecc1592e5131128499dc93a3a1cbadf14a4773500af3ac4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
