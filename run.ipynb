{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "import sys\n",
    "\n",
    "from beartype import beartype\n",
    "import datasets\n",
    "import numpy as np\n",
    "import more_itertools\n",
    "import queue\n",
    "import rich\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import wandb\n",
    "\n",
    "TokenizerType = transformers.tokenization_utils_fast.PreTrainedTokenizerFast\n",
    "\n",
    "class BaseEpsilonScheduler(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def __call__(self):\n",
    "        pass\n",
    "\n",
    "class LinearEpsilonScheduler(BaseEpsilonScheduler):\n",
    "    def __init__(self, epsilon, num_steps):\n",
    "        self.epsilon = epsilon\n",
    "        self.num_steps = num_steps\n",
    "        self.epoch = 0\n",
    "\n",
    "    def __call__(self):\n",
    "        self.epoch += 1\n",
    "        epsilon = min(self.epsilon * (1 - self.epoch / self.num_epochs), 1)\n",
    "        wandb.log({\"epsilon\": epsilon})\n",
    "        wandb.log({\"epsilon_num_steps\": self.num_steps})\n",
    "        return epsilon\n",
    "\n",
    "class ConstantEpsilonScheduler(BaseEpsilonScheduler):\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __call__(self):\n",
    "        epsilon = self.epsilon\n",
    "        wandb.log({\"epsilon\": epsilon})\n",
    "        return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/beartype/_util/hint/pep/utilpeptest.py:396: BeartypeDecorHintPepDeprecatedWarning: Type hint typing.List[str] deprecated by PEP 585. To resolve this, globally replace this hint by the equivalent PEP 585 type hint (e.g., \"typing.List[int]\" by \"list[int]\"). See also:\n",
      "    https://www.python.org/dev/peps/pep-0585\n",
      "  warn(warning_message, BeartypeDecorHintPepDeprecatedWarning)\n",
      "/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/beartype/_util/hint/pep/utilpeptest.py:396: BeartypeDecorHintPepDeprecatedWarning: Type hint typing.List[int] deprecated by PEP 585. To resolve this, globally replace this hint by the equivalent PEP 585 type hint (e.g., \"typing.List[int]\" by \"list[int]\"). See also:\n",
      "    https://www.python.org/dev/peps/pep-0585\n",
      "  warn(warning_message, BeartypeDecorHintPepDeprecatedWarning)\n"
     ]
    }
   ],
   "source": [
    "class BaseRetriever(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def retrieve(self, query_ids, query_index):\n",
    "        pass\n",
    "\n",
    "\n",
    "class StupidRetriever(BaseRetriever): \n",
    "    @beartype\n",
    "    def __init__(\n",
    "        self, *, model, tokenizer: TokenizerType, device: Union[int, str], \n",
    "        train_vectors: torch.Tensor, train_samples: List[str], train_labels: List[int]\n",
    "    ):\n",
    "    \n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.train_vectors = train_vectors\n",
    "        self.train_samples = train_samples\n",
    "        self.train_labels = train_labels\n",
    "        self.classification_ids_to_idx = {}\n",
    "\n",
    "        for i, sample in enumerate(train_samples):\n",
    "            encoded = tokenizer.encode(sample, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "            self.classification_ids_to_idx[encoded] = i\n",
    "\n",
    "    def retrieve(self, query_ids, query_index):\n",
    "        # Get the representation\n",
    "        representation = self.train_vectors[query_index]\n",
    "        with torch.inference_mode():\n",
    "            # Compute the inner products\n",
    "            scores = torch.matmul(representation, self.train_vectors.t())\n",
    "            # Get the top 2 results, to potentially exclude the sample itself.\n",
    "            topk = torch.topk(scores, k=2, dim=-1)\n",
    "        topk = topk.indices.cpu().numpy()\n",
    "        \n",
    "        for retrieved_idx in topk:\n",
    "            if retrieved_idx != query_index:\n",
    "                return self.train_samples[retrieved_idx], self.train_labels[retrieved_idx], retrieved_idx\n",
    "        \n",
    "\n",
    "# build train vectors\n",
    "@beartype\n",
    "def make_retrival_model_and_vectors(\n",
    "    retriever_name: str, path_to_vectors: Union[str, Path], device: int, dataset_name: str\n",
    "):\n",
    "    \"\"\"We expect the dir to have the following structure:\n",
    "    - config.json\n",
    "    - train_samples.json \n",
    "    - train_vectors.npy\n",
    "    \"\"\"    \n",
    "    # Make some checks\n",
    "    config =  json.loads((path_to_vectors / \"config.json\").read_text())\n",
    "    assert dataset_name == config[\"dataset_name\"], (dataset_name, config[\"dataset_name\"])\n",
    "    assert retriever_name == config[\"retriever_name\"], (retriever_name, config[\"retriever_name\"])\n",
    "\n",
    "    retriever_model = transformers.AutoModel.from_pretrained(retriever_name)\n",
    "    retriever_tokenizer = transformers.AutoTokenizer.from_pretrained(retriever_name)\n",
    "\n",
    "    with open(path_to_vectors / \"train_samples.json\") as f:\n",
    "        train_samples = json.load(f)\n",
    "        \n",
    "    vectors = torch.tensor(np.load(path_to_vectors / \"train_vectors.npy\")).to(device)\n",
    "    retriever = StupidRetriever(\n",
    "        model=retriever_model, \n",
    "        tokenizer=retriever_tokenizer, \n",
    "        device=device, \n",
    "        train_vectors=vectors, \n",
    "        train_samples=train_samples[\"inputs\"],\n",
    "        train_labels=train_samples[\"labels\"],\n",
    "    )\n",
    "    \n",
    "    return retriever\n",
    "\n",
    "\n",
    "@dataclass(order=True)\n",
    "class PrioritizedItem:\n",
    "    priority: int\n",
    "    item: Any=field(compare=False)\n",
    "\n",
    "\n",
    "class BoostingIterator(torch.utils.data.IterableDataset):\n",
    "    @beartype\n",
    "    def __init__(\n",
    "        self, \n",
    "        *args, \n",
    "        dataset, \n",
    "        retriever_client: BaseRetriever, \n",
    "        classifier: nn.Module, seed: int, \n",
    "        classification_device: Union[int, str], \n",
    "        classification_tokenizer: TokenizerType, \n",
    "        retriever_device: Union[int, str],\n",
    "        epsilon_scheduler: BaseEpsilonScheduler, \n",
    "        loss_ema_alpha: float, \n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.dataset = dataset.map(\n",
    "            lambda example, idx:{\"index\": idx}, with_indices=True\n",
    "        ).shuffle(seed=seed)\n",
    "        self.priority_queue = queue.PriorityQueue()\n",
    "        self.retriever_client = retriever_client\n",
    "        self.epsilon_scheduler = epsilon_scheduler\n",
    "        self.randomizer = np.random.RandomState(seed)\n",
    "        self.seed = seed\n",
    "        self.dataset_iter = None\n",
    "        self.classifier = classifier\n",
    "        self.classification_tokenizer = classification_tokenizer\n",
    "        self.classification_device = classification_device\n",
    "        self.retriever_device = retriever_device\n",
    "        self.loss_moving_average = None\n",
    "        self.loss_ema_alpha = loss_ema_alpha\n",
    "\n",
    "        # assert mode in [\"epsilon_priority_no_reset\", \"pure_sampled\", \"epsilon_sampled\"], mode\n",
    "\n",
    "    def push_score(self, inputs, loss):\n",
    "        average_loss = loss.mean()\n",
    "        if self.loss_moving_average is None:\n",
    "            self.loss_moving_average = average_loss\n",
    "        else:\n",
    "            self.loss_moving_average = (\n",
    "                self.loss_ema_alpha * self.loss_moving_average + (1 - self.loss_ema_alpha) * average_loss\n",
    "            )\n",
    "\n",
    "        for input_, mask, loss_, index in (\n",
    "            more_itertools.zip_equal(inputs[\"input_ids\"], inputs[\"attention_mask\"], loss, inputs[\"index\"])\n",
    "        ):\n",
    "            assert loss_.shape == torch.Size([]), loss_.shape\n",
    "            self.priority_queue.put(\n",
    "                PrioritizedItem(\n",
    "                    priority= -loss_.detach().cpu().numpy() / self.loss_moving_average, \n",
    "                    item=dict(input_ids=input_, attention_mask=mask, index=index)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        rich.print(\"[bold green]ITER[/]\")\n",
    "        self.dataset = self.dataset.shuffle(seed=self.seed)\n",
    "        self.dataset_iter = iter(self.dataset)\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        \"\"\" This is where the sampling happens.\n",
    "        \"\"\"\n",
    "\n",
    "        # Test if we have a sample and if we pass the epsilon threshold\n",
    "        empty = self.priority_queue.empty()\n",
    "        rand = self.randomizer.rand()\n",
    "        if not empty and rand < self.epsilon_scheduler():\n",
    "            # pull a sample from the priority queue\n",
    "            sample = self.priority_queue.get().item\n",
    "\n",
    "            # We retrieve the next sample.\n",
    "            input_, next_label, index = self.retriever_client.retrieve(\n",
    "                sample[\"input_ids\"], sample[\"index\"]\n",
    "            )\n",
    "            next_sample = dict(text=input_, label=next_label, index=index)\n",
    "        else:\n",
    "            next_sample = next(self.dataset_iter)  # We raise here if we have no more samples in the dataset\n",
    "            assert next_sample.keys() == {\"text\", \"label\", \"index\"}, next_sample.keys()\n",
    "\n",
    "        tokenized = self.classification_tokenizer.encode_plus(\n",
    "            next_sample[\"text\"].strip(), \n",
    "            truncation=True, \n",
    "            padding=True,\n",
    "        )\n",
    "\n",
    "        # text is not needed anymore\n",
    "        del next_sample[\"text\"]\n",
    "        assert len(tokenized.keys() & next_sample.keys()) == 0, (tokenized.keys(), next_sample.keys()) \n",
    "        return dict(**tokenized, **next_sample)\n",
    "\n",
    "\n",
    "class BoostingTrainer(transformers.Trainer):\n",
    "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "        Subclass and override to inject custom behavior.\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "                The dictionary will be unpacked before being fed to the model. \n",
    "                Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "        Return:\n",
    "            `torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        index = inputs[\"index\"]\n",
    "        # Compute loss doesn't work with extra arguments.\n",
    "        del inputs[\"index\"]\n",
    "\n",
    "        with self.autocast_smart_context_manager():\n",
    "            # Get the loss\n",
    "            loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            # Mean over per gpu averages\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "        # This is ignored in the priority queue computation\n",
    "        if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:\n",
    "            # Deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "        if self.do_grad_scaling:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        elif self.use_apex:\n",
    "            with torch.cuda.amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        elif self.deepspeed:\n",
    "            # loss gets scaled under gradient_accumulation_steps in deepspeed\n",
    "            loss = self.deepspeed.backward(loss)\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        loss = loss.detach()\n",
    "\n",
    "        # Addition for RetroBoost\n",
    "        # Make sure the losses are similar, then push them to the priority queue\n",
    "        # Put index back in\n",
    "\n",
    "        inputs[\"index\"] = index\n",
    "\n",
    "        computed_loss = torch.nn.functional.cross_entropy(outputs.logits.detach(), inputs[\"labels\"].detach(), reduction=\"mean\")\n",
    "        loss_per_sample = torch.nn.functional.cross_entropy(outputs.logits.detach(), inputs[\"labels\"].detach(), reduction=\"none\")\n",
    "        assert torch.allclose(loss, computed_loss, atol=0.1)\n",
    "        self.get_train_dataloader().dataset.push_score(inputs, loss_per_sample)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mretroboost\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/julesgm/RetroBoost/runs/2nv8co6i\" target=\"_blank\">baseline-trainer</a></strong> to <a href=\"https://wandb.ai/julesgm/RetroBoost\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/home/mila/g/gagnonju/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8a25faa3ff4871abf7794641d283e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/home/mila/g/gagnonju/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5df87b05efd45c3a7aecfd749d52428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535863ca5e324560a1471d459b0a8d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RETRIEVER_NAME = \"facebook/contriever\"\n",
    "DATASET_NAME = \"ag_news\"\n",
    "PATH_TO_VECTORS = Path(f\"./vectors_{DATASET_NAME}_{RETRIEVER_NAME.split('/')[-1]}/\")\n",
    "CLASSIFIER_NAME = \"roberta-base\"\n",
    "CLASSIFIER_BATCH_SIZE = 40\n",
    "EPSILON_SCHEDULER_TYPE = \"constant\"\n",
    "EPSILON_SCHEDULER_CONFIG = dict(\n",
    "    epsilon=0.5,\n",
    ")\n",
    "LOSS_EMA_ALPHA = 0.5\n",
    "REGULAR_TRAINER = True\n",
    "\n",
    "CLASSIFIER_DEVICE = 1\n",
    "RETRIEVER_DEVICE = 2\n",
    "SEED = 0\n",
    "SPLIT_RATIO = 0.85\n",
    "\n",
    "###############################################################################\n",
    "# Fast setup \n",
    "###############################################################################\n",
    "config = dict(\n",
    "        classifier_batch_size=CLASSIFIER_BATCH_SIZE,\n",
    "        classifier_name=CLASSIFIER_NAME,\n",
    "        dataset_name=DATASET_NAME,\n",
    "        epsilon=dict(\n",
    "            scheduler_type=EPSILON_SCHEDULER_TYPE,\n",
    "            scheduler_config=EPSILON_SCHEDULER_CONFIG,\n",
    "        ),\n",
    "        loss_ema_alpha=LOSS_EMA_ALPHA,\n",
    "        random_seed=SEED,\n",
    "        regular_trainer=REGULAR_TRAINER,\n",
    "        retriever_name=RETRIEVER_NAME,\n",
    "        split_ratio=SPLIT_RATIO,\n",
    "    )\n",
    "\n",
    "wandb.init(\n",
    "    config=config,\n",
    "    project=\"RetroBoost\", \n",
    "    entity=\"julesgm\",\n",
    "    name=\"baseline-trainer\",\n",
    ")\n",
    "\n",
    "EPSILON_SCHEDULER_TYPE_MAP = dict(\n",
    "    constant=ConstantEpsilonScheduler,\n",
    ")\n",
    "\n",
    "# Random seeds. \n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "dataset = datasets.load_dataset(DATASET_NAME)\n",
    "ALL_LABELS = set(dataset[\"train\"][\"label\"])\n",
    "NUM_LABELS = len(ALL_LABELS)\n",
    "assert ALL_LABELS == set(range(NUM_LABELS))\n",
    "\n",
    "classifier_name = CLASSIFIER_NAME\n",
    "dataset_name = DATASET_NAME\n",
    "regular_trainer = REGULAR_TRAINER\n",
    "\n",
    "\n",
    "classifier = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    classifier_name, num_labels=NUM_LABELS\n",
    ")\n",
    "classifier_tokenizer = transformers.AutoTokenizer.from_pretrained(classifier_name)\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "\n",
    "dataset_train = datasets.load_dataset(DATASET_NAME, split=f\"train[:{SPLIT_RATIO:.0%}]\")\n",
    "dataset_validation = datasets.load_dataset(DATASET_NAME, split=f\"train[{SPLIT_RATIO:.0%}:]\")\n",
    "\n",
    "tokenized_training = dataset_train.map(\n",
    "    lambda examples: preprocess_function(examples, classifier_tokenizer), \n",
    "    batched=True\n",
    ").shuffle(seed=SEED)\n",
    "\n",
    "tokenized_validation = dataset_validation.map(\n",
    "    lambda examples: preprocess_function(examples, classifier_tokenizer), \n",
    "    batched=True\n",
    ").shuffle(seed=SEED)\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=CLASSIFIER_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=int(CLASSIFIER_BATCH_SIZE * 1.5),\n",
    "    num_train_epochs=10,\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = make_retrival_model_and_vectors(\n",
    "    retriever_name=RETRIEVER_NAME, \n",
    "    path_to_vectors=PATH_TO_VECTORS, \n",
    "    device=RETRIEVER_DEVICE, \n",
    "    dataset_name=DATASET_NAME,\n",
    ")\n",
    "retriever_client = retriever\n",
    "\n",
    "if regular_trainer:\n",
    "    trainer = transformers.Trainer(\n",
    "        model=classifier.to(CLASSIFIER_DEVICE), \n",
    "        args=training_args, \n",
    "        tokenizer=classifier_tokenizer, \n",
    "        train_dataset=tokenized_training, \n",
    "        eval_dataset=tokenized_validation,\n",
    "        data_collator=transformers.DataCollatorWithPadding(\n",
    "            tokenizer=classifier_tokenizer\n",
    "        ),\n",
    "    )\n",
    "else:\n",
    "    tokenized_training = BoostingIterator(\n",
    "        dataset=dataset[\"train\"], \n",
    "        retriever_client=retriever_client, \n",
    "        classifier=classifier, \n",
    "        epsilon_scheduler=EPSILON_SCHEDULER_TYPE_MAP[EPSILON_SCHEDULER_TYPE](**EPSILON_SCHEDULER_CONFIG), \n",
    "        seed=SEED,\n",
    "        retriever_device=RETRIEVER_DEVICE, \n",
    "        classification_device=CLASSIFIER_DEVICE,\n",
    "        classification_tokenizer=classifier_tokenizer,\n",
    "        loss_ema_alpha=LOSS_EMA_ALPHA,\n",
    "    )\n",
    "    \n",
    "    trainer = BoostingTrainer(\n",
    "        model=classifier.to(CLASSIFIER_DEVICE),\n",
    "        args=training_args, \n",
    "        tokenizer=classifier_tokenizer, \n",
    "        train_dataset=tokenized_training, \n",
    "        eval_dataset=tokenized_validation,\n",
    "        data_collator=transformers.DataCollatorWithPadding(\n",
    "            tokenizer=classifier_tokenizer\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 40\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 120\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_397548/3431184757.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.main/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1339\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m                 \u001b[0;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.main/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.main/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.main/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.main/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "output = trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.StupidRetriever at 0x7fdfcd28fac0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datasets.arrow_dataset.Dataset,\n",
       " datasets.arrow_dataset.DatasetInfoMixin,\n",
       " datasets.search.IndexableMixin,\n",
       " datasets.arrow_dataset.TensorflowDatasetMixin,\n",
       " object]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AUTHORS',\n",
       " 'AUTHOR_EMAIL',\n",
       " 'COPYRIGHT',\n",
       " 'LIBS_DEVELOPER_MANDATORY',\n",
       " 'LIBS_DOCTIME_MANDATORY',\n",
       " 'LIBS_DOCTIME_MANDATORY_RTD',\n",
       " 'LIBS_RUNTIME_OPTIONAL',\n",
       " 'LIBS_TESTTIME_MANDATORY',\n",
       " 'LIBS_TESTTIME_MANDATORY_COVERAGE',\n",
       " 'LIBS_TESTTIME_MANDATORY_TOX',\n",
       " 'LIBS_TESTTIME_OPTIONAL',\n",
       " 'LICENSE',\n",
       " 'NAME',\n",
       " 'PACKAGE_NAME',\n",
       " 'PYTHON_VERSION_MIN',\n",
       " 'PYTHON_VERSION_MINOR_MAX',\n",
       " 'PYTHON_VERSION_MIN_PARTS',\n",
       " 'SYNOPSIS',\n",
       " 'URL_DOWNLOAD',\n",
       " 'URL_HOMEPAGE',\n",
       " 'URL_ISSUES',\n",
       " 'URL_RELEASES',\n",
       " 'VERSION',\n",
       " 'VERSION_PARTS',\n",
       " '_LIB_RUNTIME_OPTIONAL_VERSION_MINIMUM_NUMPY',\n",
       " '_LIB_RUNTIME_OPTIONAL_VERSION_MINIMUM_TYPING_EXTENSIONS',\n",
       " '_LIB_VERSION_MIN_SPHINX',\n",
       " '_LIB_VERSION_MIN_SPHINX_RTD_THEME',\n",
       " '_PYTHON_VERSION_PARTS',\n",
       " '_Tuple',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_convert_version_str_to_tuple',\n",
       " '_is_os_macos',\n",
       " '_is_py_pypy',\n",
       " '_sys']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(bt.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/mila/g/gagnonju/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
      "Reusing dataset imdb (/home/mila/g/gagnonju/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    }
   ],
   "source": [
    "# tokenized_training = dataset_tok[:int(SPLIT_RATIO * len(dataset_tok))]\n",
    "# tokenized_validation = dataset_tok[int(SPLIT_RATIO * len(dataset_tok)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I really can say I felt the movie in its right essence where the mind games from dreamy reality enter into the surreal aspect of future faced by Tom Cruise. I didn\\'t cared much about Tom Cruise\\'s acting prowess but I must say that he seems to impress at every point in the movie...not simply due to an engaging storyline but also due to his self being imparted to the lead character....they merge and then speak and its beautiful. However I must say this movie doesn\\'t come under the \"average flick of weekend\" which you pick at random and watch gleefully; It carries strong sentiments and characters so don\\'t wash this one down with your beer and pop-corns. It certainly needs more than that.',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column test not in the dataset. Current columns in the dataset: ['text', 'label']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_397548/2444897559.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.main/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1923\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: F811\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1924\u001b[0m         \u001b[0;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1925\u001b[0;31m         return self._getitem(\n\u001b[0m\u001b[1;32m   1926\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m         )\n",
      "\u001b[0;32m~/.main/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[1;32m   1907\u001b[0m         \u001b[0mformat_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_kwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mformat_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1909\u001b[0;31m         \u001b[0mpa_subtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1910\u001b[0m         formatted_output = format_table(\n\u001b[1;32m   1911\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.main/lib/python3.9/site-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0m_raise_bad_key_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0m_check_valid_column_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.main/lib/python3.9/site-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m_check_valid_column_key\u001b[0;34m(key, columns)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_valid_column_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Column {key} not in the dataset. Current columns in the dataset: {columns}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column test not in the dataset. Current columns in the dataset: ['text', 'label']\""
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46daadc73974f0324ecc1592e5131128499dc93a3a1cbadf14a4773500af3ac4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
