{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "import sys\n",
    "\n",
    "from beartype import beartype\n",
    "import datasets\n",
    "import numpy as np\n",
    "import more_itertools\n",
    "import queue\n",
    "import rich\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import wandb\n",
    "\n",
    "TokenizerType = transformers.tokenization_utils_fast.PreTrainedTokenizerFast\n",
    "\n",
    "class BaseEpsilonScheduler(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def __call__(self):\n",
    "        pass\n",
    "\n",
    "class LinearEpsilonScheduler(BaseEpsilonScheduler):\n",
    "    def __init__(self, epsilon, num_steps):\n",
    "        self.epsilon = epsilon\n",
    "        self.num_steps = num_steps\n",
    "        self.epoch = 0\n",
    "\n",
    "    def __call__(self):\n",
    "        self.epoch += 1\n",
    "        epsilon = min(self.epsilon * (1 - self.epoch / self.num_epochs), 1)\n",
    "        wandb.log({\"epsilon\": epsilon})\n",
    "        wandb.log({\"epsilon_num_steps\": self.num_steps})\n",
    "        return epsilon\n",
    "\n",
    "class ConstantEpsilonScheduler(BaseEpsilonScheduler):\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __call__(self):\n",
    "        epsilon = self.epsilon\n",
    "        wandb.log({\"epsilon\": epsilon})\n",
    "        return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRetriever(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def retrieve(self, query_ids, query_index):\n",
    "        pass\n",
    "\n",
    "\n",
    "class StupidRetriever(BaseRetriever): \n",
    "    @beartype\n",
    "    def __init__(\n",
    "        self, *, model, tokenizer: TokenizerType, device: Union[int, str], \n",
    "        train_vectors: torch.Tensor, train_samples_dict: Dict[str, Any],\n",
    "    ):\n",
    "    \n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.train_vectors = train_vectors\n",
    "        self.train_samples_dict = train_samples_dict\n",
    "\n",
    "    def retrieve(self, query_index):\n",
    "        # Get the representation\n",
    "        representation = self.train_vectors[query_index]\n",
    "        with torch.inference_mode():\n",
    "            # Compute the inner products\n",
    "            scores = torch.matmul(representation, self.train_vectors.t())\n",
    "            # Get the top 2 results, to potentially exclude the sample itself.\n",
    "            topk = torch.topk(scores, k=2, dim=-1)\n",
    "        topk = topk.indices.cpu().numpy()\n",
    "        \n",
    "        for retrieved_idx in topk:\n",
    "            if retrieved_idx != query_index:\n",
    "                return {k: v[retrieved_idx] for k, v in self.train_samples_dict.items()} | {\"index\": retrieved_idx}\n",
    "        \n",
    "# build train vectors\n",
    "@beartype\n",
    "def make_retrival_model_and_vectors(\n",
    "    retriever_name: str, path_to_vectors: Union[str, Path], device: int, dataset_type: str,\n",
    "):\n",
    "    \"\"\"We expect the dir to have the following structure:\n",
    "    - config.json\n",
    "    - train_samples.json \n",
    "    - train_vectors.npy\n",
    "    \"\"\"    \n",
    "    # Make some checks\n",
    "    retriever_model = transformers.AutoModel.from_pretrained(retriever_name)\n",
    "    retriever_tokenizer = transformers.AutoTokenizer.from_pretrained(retriever_name)\n",
    "\n",
    "    with open(path_to_vectors / \"train_samples.json\") as f:\n",
    "        train_samples_dict = json.load(f)\n",
    "        \n",
    "\n",
    "    vectors = torch.tensor(np.load(path_to_vectors / \"train_vectors.npy\")).to(device)\n",
    "    retriever = StupidRetriever(\n",
    "        model=retriever_model, \n",
    "        tokenizer=retriever_tokenizer, \n",
    "        device=device, \n",
    "        train_vectors=vectors, \n",
    "        train_samples_dict=train_samples_dict,\n",
    "    )\n",
    "    \n",
    "    return retriever\n",
    "\n",
    "\n",
    "@dataclass(order=True)\n",
    "class PrioritizedItem:\n",
    "    priority: int\n",
    "    item: Any=field(compare=False)\n",
    "\n",
    "\n",
    "class BoostingIterator(torch.utils.data.IterableDataset):\n",
    "    @beartype\n",
    "    def __init__(\n",
    "        self, \n",
    "        *, \n",
    "        dataset, \n",
    "        retriever_client: BaseRetriever, \n",
    "        classifier: nn.Module, seed: int, \n",
    "        classification_device: Union[int, str], \n",
    "        classification_tokenizer: TokenizerType, \n",
    "        retriever_device: Union[int, str],\n",
    "        epsilon_scheduler: BaseEpsilonScheduler, \n",
    "        loss_ema_alpha: float, \n",
    "        config: Dict[str, Any],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset.map(\n",
    "            lambda example, idx:{\"index\": idx}, with_indices=True, \n",
    "        ).shuffle(seed=seed)\n",
    "        self.dataset = self.dataset.remove_columns([\"idx\"])\n",
    "        self.priority_queue = queue.PriorityQueue()\n",
    "        self.retriever_client = retriever_client\n",
    "        self.epsilon_scheduler = epsilon_scheduler\n",
    "        self.randomizer = np.random.RandomState(seed)\n",
    "        self.seed = seed\n",
    "        self.dataset_iter = None\n",
    "        self.classifier = classifier\n",
    "        self.classification_tokenizer = classification_tokenizer\n",
    "        self.classification_device = classification_device\n",
    "        self.retriever_device = retriever_device\n",
    "        self.loss_moving_average = None\n",
    "        self.loss_ema_alpha = loss_ema_alpha\n",
    "        self.dataset_type = config[\"dataset_type\"]\n",
    "        if self.dataset_type == \"dual_entry_classification\":\n",
    "            self.field_a_name = config[\"field_a_name\"]\n",
    "            self.field_b_name = config[\"field_b_name\"]\n",
    "\n",
    "        assert \"idx\" not in self.dataset\n",
    "\n",
    "        # assert mode in [\"epsilon_priority_no_reset\", \"pure_sampled\", \"epsilon_sampled\"], mode\n",
    "\n",
    "    def push_score(self, inputs, loss):\n",
    "        average_loss = loss.mean()\n",
    "        if self.loss_moving_average is None:\n",
    "            self.loss_moving_average = average_loss\n",
    "        else:\n",
    "            self.loss_moving_average = (\n",
    "                self.loss_ema_alpha * self.loss_moving_average + (1 - self.loss_ema_alpha) * average_loss\n",
    "            )\n",
    "\n",
    "        for input_, mask, loss_, index in (\n",
    "            more_itertools.zip_equal(inputs[\"input_ids\"], inputs[\"attention_mask\"], loss, inputs[\"index\"])\n",
    "        ):\n",
    "            assert loss_.shape == torch.Size([]), loss_.shape\n",
    "            self.priority_queue.put(\n",
    "                PrioritizedItem(\n",
    "                    priority= -loss_.detach().cpu().numpy() / self.loss_moving_average, \n",
    "                    item=dict(input_ids=input_, attention_mask=mask, index=index)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        rich.print(\"[bold green]ITER[/]\")\n",
    "        self.dataset = self.dataset.shuffle(seed=self.seed)\n",
    "        self.dataset_iter = iter(self.dataset)\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        \"\"\" This is where the sampling happens.\n",
    "        \"\"\"\n",
    "\n",
    "        # Test if we have a sample and if we pass the epsilon threshold\n",
    "        empty = self.priority_queue.empty()\n",
    "        rand = self.randomizer.rand()\n",
    "        if not empty and rand < self.epsilon_scheduler():\n",
    "            # pull a sample from the priority queue\n",
    "            sample = self.priority_queue.get().item\n",
    "\n",
    "            # We retrieve the next sample.\n",
    "            next_sample = self.retriever_client.retrieve(sample[\"index\"])\n",
    "        else:\n",
    "            next_sample = next(self.dataset_iter)  # We raise here if we have no more samples in the dataset\n",
    "\n",
    "\n",
    "        if self.dataset_type == \"single_entry_classification\":\n",
    "            tokenized = self.classification_tokenizer.encode_plus(\n",
    "                next_sample[\"inputs\"], \n",
    "                truncation=True, \n",
    "                padding=True,    \n",
    "            )\n",
    "            del next_sample[\"inputs\"]\n",
    "        elif self.dataset_type == \"dual_entry_classification\":\n",
    "            tokenized = self.classification_tokenizer.encode_plus(\n",
    "                next_sample[self.field_a_name], \n",
    "                next_sample[self.field_b_name], \n",
    "                truncation=True, \n",
    "                padding=True,\n",
    "            )\n",
    "            del next_sample[self.field_a_name]\n",
    "            del next_sample[self.field_b_name]\n",
    "            # if \"idx\" in next_sample:\n",
    "            #     del next_sample[\"idx\"]\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dataset type: {self.dataset_type}\")\n",
    "\n",
    "        # text is not needed anymore\n",
    "        \n",
    "        assert len(tokenized.keys() & next_sample.keys()) == 0, (tokenized.keys(), next_sample.keys()) \n",
    "        return dict(**tokenized, **next_sample)\n",
    "\n",
    "\n",
    "class BoostingTrainer(transformers.Trainer):\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        assert \"labels\" in inputs, inputs.keys()\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        assert labels is None\n",
    "\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        rich.print(f\"[green bold]{outputs}\")\n",
    "        # Save past state if it exists\n",
    "        # TODO: this needs to be fixed and made cleaner later.\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            assert False\n",
    "            loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "            rich.print(f\"[yellow bold]{outputs[1].shape}\")\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "        Subclass and override to inject custom behavior.\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "                The dictionary will be unpacked before being fed to the model. \n",
    "                Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "        Return:\n",
    "            `torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        index = inputs[\"index\"]\n",
    "        # Compute loss doesn't work with extra arguments.\n",
    "        del inputs[\"index\"]\n",
    "\n",
    "        with self.autocast_smart_context_manager():\n",
    "            # Get the loss\n",
    "            loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            # Mean over per gpu averages\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "        # This is ignored in the priority queue computation\n",
    "        if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:\n",
    "            assert False\n",
    "            # Deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "        \n",
    "        if self.do_grad_scaling:\n",
    "            assert False\n",
    "            self.scaler.scale(loss).backward()\n",
    "        elif self.use_apex:\n",
    "            assert False\n",
    "            with torch.cuda.amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        elif self.deepspeed:\n",
    "            assert False\n",
    "            # loss gets scaled under gradient_accumulation_steps in deepspeed\n",
    "            loss = self.deepspeed.backward(loss)\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        loss = loss.detach()\n",
    "\n",
    "        # Addition for RetroBoost\n",
    "        # Make sure the losses are similar, then push them to the priority queue\n",
    "        # Put index back in\n",
    "\n",
    "        inputs[\"index\"] = index\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            loss_per_sample = torch.nn.functional.cross_entropy(outputs.logits.detach(), inputs[\"labels\"].detach(), reduction=\"none\")\n",
    "            assert loss_per_sample.ndim == 1, loss_per_sample.ndim\n",
    "            loss_per_gpu = torch.mean(loss_per_sample, dim=0)\n",
    "            computed_loss = torch.mean(loss_per_gpu)\n",
    "            rich.print(\"[red bold]logits[/]\", outputs.logits.detach().cpu().numpy())\n",
    "            rich.print(\"[red bold]logits[/]\", outputs.logits.detach().cpu().numpy().shape)\n",
    "            rich.print(\"[red bold]LOSS[/]\", loss.detach().cpu().numpy(), \" [red bold]computed_loss[/]\", computed_loss)\n",
    "            assert torch.allclose(loss, computed_loss, atol=0.01)\n",
    "\n",
    "            self.get_train_dataloader().dataset.push_score(inputs, loss_per_sample)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1tdmjq4l) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1475529... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366850242b3348ccb53ec5a05ad679af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.14MB of 0.14MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">baseline-trainer</strong>: <a href=\"https://wandb.ai/julesgm/RetroBoost/runs/1tdmjq4l\" target=\"_blank\">https://wandb.ai/julesgm/RetroBoost/runs/1tdmjq4l</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220212_011019-1tdmjq4l/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1tdmjq4l). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/julesgm/RetroBoost/runs/3gvqdc9n\" target=\"_blank\">baseline-trainer</a></strong> to <a href=\"https://wandb.ai/julesgm/RetroBoost\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Reusing dataset super_glue (/home/mila/g/gagnonju/.cache/huggingface/datasets/super_glue/rte/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n",
      "Reusing dataset super_glue (/home/mila/g/gagnonju/.cache/huggingface/datasets/super_glue/rte/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n",
      "Loading cached processed dataset at /home/mila/g/gagnonju/.cache/huggingface/datasets/super_glue/rte/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7/cache-a4aecc1a46c818a7.arrow\n",
      "Loading cached shuffled indices for dataset at /home/mila/g/gagnonju/.cache/huggingface/datasets/super_glue/rte/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7/cache-37257e31646103cc.arrow\n",
      "Loading cached processed dataset at /home/mila/g/gagnonju/.cache/huggingface/datasets/super_glue/rte/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7/cache-a1fd86c05086b1ae.arrow\n",
      "Loading cached shuffled indices for dataset at /home/mila/g/gagnonju/.cache/huggingface/datasets/super_glue/rte/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7/cache-aa66d1fec0f29c2d.arrow\n",
      "PyTorch: setting up devices\n",
      "loading configuration file https://huggingface.co/facebook/contriever/resolve/main/config.json from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/52956acf642ee38953ffe7ea253f16896223c887093b1cbbdcec52c82ed6ea6c.b19e5a240d78d865c8542d06f7a66ec533465c843c39fa863e76ebb42cd7a581\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"facebook/contriever\",\n",
      "  \"architectures\": [\n",
      "    \"Contriever\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/facebook/contriever/resolve/main/pytorch_model.bin from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/8536bee11f14ad7063b2489c148f67a958ab83ab3b727ae6a1846a6eb7d86ba4.b0886cf70b0752007a1d3a6161622704be1f74c942b77bdfec51570558c35264\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at facebook/contriever.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file https://huggingface.co/facebook/contriever/resolve/main/vocab.txt from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/be0b434deb0e60a3e1dbef9cd5a007ab66ccc34a6caaf37bb261ad28663449a1.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/facebook/contriever/resolve/main/tokenizer.json from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/46f1d52bf62c9485839492be3ddff22058e3d76592e1c6a338e28061c6c711bd.f471bd2d72c48b932f7be40446896b7e97c3be406ee93abfb500399bc606c829\n",
      "loading file https://huggingface.co/facebook/contriever/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/facebook/contriever/resolve/main/special_tokens_map.json from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/ac7a4b49e74445ae37291a5057a526b64b9a80a70ffa9d27dc756980bfa9bb1b.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/facebook/contriever/resolve/main/tokenizer_config.json from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/786ef1d235d43b5473c1bc5e6ac3c98f8603ef39456124f54a1df2718a0cb2e8.59407384618422b5f582b6046df91db98a0f921d6c959dc7b1f50000ffea1032\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/mila/g/gagnonju/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "RETRIEVER_NAME = \"facebook/contriever\"\n",
    "DATASET_TUPLE = (\"super_glue\", \"rte\")\n",
    "TASK_TYPE = \"dual_entry_classification\"\n",
    "PATH_TO_VECTORS = Path(f\"./vectors_{'_'.join(DATASET_TUPLE)}_{RETRIEVER_NAME.split('/')[-1]}/\")\n",
    "CLASSIFIER_NAME = \"roberta-base\"\n",
    "CLASSIFIER_BATCH_SIZE = 20\n",
    "EPSILON_SCHEDULER_TYPE = \"constant\"\n",
    "EPSILON_SCHEDULER_CONFIG = dict(\n",
    "    epsilon=0.5,\n",
    ")\n",
    "LOSS_EMA_ALPHA = 0.5\n",
    "REGULAR_TRAINER = False\n",
    "WEIGHT_DECAY = 0.05\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "CLASSIFIER_EVAL_BATCH_SIZE_MULTIPLIER = 1.5\n",
    "CLASSIFIER_DEVICE = 1\n",
    "RETRIEVER_DEVICE = 2\n",
    "SEED = 0\n",
    "SPLIT_RATIO = 0.85\n",
    "NUM_EPOCHS_TO_TRAIN_ON = 30\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Fast setup \n",
    "###############################################################################\n",
    "wandb_config = dict(\n",
    "        classifier_batch_size=CLASSIFIER_BATCH_SIZE,\n",
    "        classifier_name=CLASSIFIER_NAME,\n",
    "        dataset_tuple=DATASET_TUPLE,\n",
    "        epsilon=dict(\n",
    "            scheduler_type=EPSILON_SCHEDULER_TYPE,\n",
    "            scheduler_config=EPSILON_SCHEDULER_CONFIG,\n",
    "        ),\n",
    "        loss_ema_alpha=LOSS_EMA_ALPHA,\n",
    "        random_seed=SEED,\n",
    "        regular_trainer=REGULAR_TRAINER,\n",
    "        retriever_name=RETRIEVER_NAME,\n",
    "        split_ratio=SPLIT_RATIO,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "    )\n",
    "\n",
    "wandb.init(\n",
    "    config=wandb_config,\n",
    "    project=\"RetroBoost\", \n",
    "    entity=\"julesgm\",\n",
    "    name=\"baseline-trainer\",\n",
    ")\n",
    "\n",
    "EPSILON_SCHEDULER_TYPE_MAP = dict(\n",
    "    constant=ConstantEpsilonScheduler,\n",
    ")\n",
    "\n",
    "# Random seeds. \n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "classifier_tokenizer: Final = transformers.AutoTokenizer.from_pretrained(CLASSIFIER_NAME)\n",
    "\n",
    "# Load the config\n",
    "config: Final = json.loads((PATH_TO_VECTORS / \"config.json\").read_text())\n",
    "assert config[\"retriever_name\"] == RETRIEVER_NAME, f\"{config['retriever_name']} != {RETRIEVER_NAME}\"\n",
    "\n",
    "# Load the datasets\n",
    "dataset_train: Final = datasets.load_dataset(*DATASET_TUPLE, split=f\"train[:{SPLIT_RATIO:.0%}]\")\n",
    "dataset_validation: Final = datasets.load_dataset(*DATASET_TUPLE, split=f\"train[{SPLIT_RATIO:.0%}:]\")\n",
    "\n",
    "ALL_LABELS = set(dataset_train[\"label\"])\n",
    "NUM_LABELS = len(ALL_LABELS)\n",
    "assert ALL_LABELS == set(range(NUM_LABELS))\n",
    "\n",
    "# Delete the extra fields\n",
    "if config[\"dataset_type\"] == \"dual_entry_classification\":\n",
    "    fields = dataset_train[0].keys()\n",
    "    dataset_train.remove_columns(fields - {config[\"field_a_name\"], config[\"field_b_name\"], \"label\"} )\n",
    "\n",
    "def preprocess_function(examples, tokenizer, config):\n",
    "    if config[\"dataset_type\"] == \"single_entry_classification\":\n",
    "        return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "    elif config[\"dataset_type\"] == \"dual_entry_classification\":\n",
    "        return tokenizer(\n",
    "            examples[config[\"field_a_name\"]], \n",
    "            examples[config[\"field_b_name\"]], \n",
    "            truncation=True, \n",
    "            padding=True,\n",
    "        )\n",
    "\n",
    "    raise ValueError(f\"Unknown dataset type {config['dataset_type']}\")\n",
    "\n",
    "tokenized_training: Final = dataset_train.map(\n",
    "    lambda examples: preprocess_function(examples, classifier_tokenizer, config), \n",
    "    batched=True\n",
    ").shuffle(seed=SEED)\n",
    "\n",
    "tokenized_validation: Final = dataset_validation.map(\n",
    "    lambda examples: preprocess_function(examples, classifier_tokenizer, config), \n",
    "    batched=True\n",
    ").shuffle(seed=SEED)\n",
    "\n",
    "training_args: Final = transformers.TrainingArguments(\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=CLASSIFIER_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=int(CLASSIFIER_BATCH_SIZE * CLASSIFIER_EVAL_BATCH_SIZE_MULTIPLIER),\n",
    "    num_train_epochs=NUM_EPOCHS_TO_TRAIN_ON,\n",
    "    report_to=\"wandb\",\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "retriever: Final = make_retrival_model_and_vectors(\n",
    "    retriever_name=RETRIEVER_NAME, \n",
    "    path_to_vectors=PATH_TO_VECTORS, \n",
    "    device=RETRIEVER_DEVICE, \n",
    "    dataset_type=config[\"dataset_type\"],\n",
    ")\n",
    "retriever_client: Final = retriever\n",
    "\n",
    "classifier: Final = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    CLASSIFIER_NAME, num_labels=NUM_LABELS\n",
    ")\n",
    "\n",
    "classifier.config.problem_type = \"single_label_classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (480604505.py, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_1393057/480604505.py\"\u001b[0;36m, line \u001b[0;32m28\u001b[0m\n\u001b[0;31m    args=training_args,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return datasets.load_metric(\"accuracy\").compute(predictions=predictions, references=labels)\n",
    "\n",
    "if REGULAR_TRAINER:\n",
    "    TrainerClass = transformers.Trainer    \n",
    "    ds_train = tokenized_training\n",
    "else:\n",
    "    TrainerClass = BoostingTrainer\n",
    "    \n",
    "    ds_train = BoostingIterator(\n",
    "        dataset=dataset_train, \n",
    "        retriever_client=retriever_client, \n",
    "        classifier=classifier, \n",
    "        epsilon_scheduler=EPSILON_SCHEDULER_TYPE_MAP[EPSILON_SCHEDULER_TYPE](**EPSILON_SCHEDULER_CONFIG), \n",
    "        seed=SEED,\n",
    "        retriever_device=RETRIEVER_DEVICE, \n",
    "        classification_device=CLASSIFIER_DEVICE,\n",
    "        classification_tokenizer=classifier_tokenizer,\n",
    "        loss_ema_alpha=LOSS_EMA_ALPHA,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = TrainerClass(\n",
    "        model=classifier\n",
    "        args=training_args, \n",
    "        tokenizer=classifier_tokenizer, \n",
    "        train_dataset=ds_train, \n",
    "        eval_dataset=tokenized_validation,\n",
    "        data_collator=transformers.DataCollatorWithPadding(\n",
    "            tokenizer=classifier_tokenizer\n",
    "        ),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/g/gagnonju/.main/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2116\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 20\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 60\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3180\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">ITER</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mITER\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/mila/g/gagnonju/.cache/huggingface/datasets/super_glue/rte/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7/cache-60360dcf2c50180c.arrow\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataParallel' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1393057/3349114407.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.main/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m                 if (\n",
      "\u001b[0;32m/tmp/ipykernel_1393057/4060954328.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast_smart_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;31m# Get the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1393057/4060954328.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mrich\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[bold cyan]{type(model.model).mro()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.main/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataParallel' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "output = trainer.train()\n",
    "print(output)\n",
    "print(trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.config.problem_type is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46daadc73974f0324ecc1592e5131128499dc93a3a1cbadf14a4773500af3ac4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
